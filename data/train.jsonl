{"text": "File: workflow.py\n\nfrom backend.agents.crew_lite import Agent, Task, Crew, Process\nfrom backend.agents.config import get_llm\nfrom backend.agents.tools import AgentTools\n\nllm = get_llm()\n\n# --- Define Advanced Agents ---\n\nlead_dev = Agent(\n    role='Lead Next.js Developer',\n    goal='Write production-grade Next.js 14 code with TypeScript and TailwindCSS.',\n    backstory='You are a 10x developer. You write clean, accessible, and performant code. You adhere strictly to the \"use client\" directive where necessary.',\n    llm=llm,\n    tools=[AgentTools.write_file, AgentTools.read_file]\n)\n\nsecurity_analyst = Agent(\n    role='Application Security Analyst',\n    goal='Review code for vulnerabilities (XSS, Injection, Auth) and Approve/Reject it.',\n    backstory='You are a gatekeeper. You NEVER let insecure code pass. You check for missing validation, secrets in code, and dangerous patterns. You output \"APPROVE\" only if it is safe.',\n    llm=llm,\n    tools=[AgentTools.read_file]\n)\n\ntech_writer = Agent(\n    role='Technical Writer',\n    goal='Write comprehensive documentation and JSDoc comments.',\n    backstory='You turn complex code into easy-to-understand documentation. You update READMEs and add JSDocs.',\n    llm=llm,\n    tools=[AgentTools.write_file, AgentTools.read_file]\n)\n\ndef run_feature_workflow(feature_description: str):\n    context = \"\"\n    \n    # Step 1: Development\n    print(f\"Lead Dev working on: {feature_description}\")\n    dev_task = f\"Create a Next.js component for: {feature_description}. Save it to 'components/agents/generated/Feature.tsx'.\"\n    code_output = lead_dev.execute(dev_task, context)\n    context += f\"\\n\\n[Lead Dev]: {code_output}\"\n    \n    # Step 2: Security Review Loop (Max 1 retry for this demo)\n    max_retries = 1\n    approved = False\n    \n    for i in range(max_retries + 1):\n        print(f\"Security Review (Attempt {i+1})...\")\n        sec_task = \"Review 'components/agents/generated/Feature.tsx'. If safe, output 'APPROVE'. If unsafe, output 'REJECT' and explain why.\"\n        sec_output = security_analyst.execute(sec_task, context)\n        context += f\"\\n\\n[Security]: {sec_output}\"\n        \n        if \"APPROVE\" in sec_output.upper():\n            approved = True\n            print(\"Security Approved.\")\n            break\n        else:\n            print(\"Security Rejected. Sending back to Dev...\")\n            fix_task = f\"Fix the security issues identified: {sec_output}. Update 'components/agents/generated/Feature.tsx'.\"\n            fix_output = lead_dev.execute(fix_task, context)\n            context += f\"\\n\\n[Lead Dev - Fix]: {fix_output}\"\n    \n    if not approved:\n        return \"Workflow Failed: Security did not approve the code.\"\n\n    # Step 3: Documentation\n    print(\"Generating Documentation...\")\n    doc_task = \"Read 'components/agents/generated/Feature.tsx' and write JSDoc comments for it. Also create 'components/agents/generated/README.md'.\"\n    doc_output = tech_writer.execute(doc_task, context)\n    context += f\"\\n\\n[Tech Writer]: {doc_output}\"\n    \n    return \"Workflow Complete. Code Approved and Documented.\"\n"}
{"text": "File: tools.py\n\nfrom langchain.tools import tool\nimport os\n\nclass AgentTools:\n    @tool(\"Write File\")\n    def write_file(file_path: str, content: str):\n        \"\"\"Writes content to a file at the specified path. Use this to save code, tests, or reports.\"\"\"\n        try:\n            # Ensure directory exists\n            directory = os.path.dirname(file_path)\n            if directory and not os.path.exists(directory):\n                os.makedirs(directory)\n            \n            with open(file_path, 'w') as f:\n                f.write(content)\n            return f\"Successfully wrote to {file_path}\"\n        except Exception as e:\n            return f\"Error writing file: {str(e)}\"\n\n    @tool(\"Read File\")\n    def read_file(file_path: str):\n        \"\"\"Reads the content of a file.\"\"\"\n        try:\n            if not os.path.exists(file_path):\n                return \"File not found.\"\n            with open(file_path, 'r') as f:\n                return f.read()\n        except Exception as e:\n            return f\"Error reading file: {str(e)}\"\n    \n    @tool(\"List Directory\")\n    def list_dir(path: str = \".\"):\n        \"\"\"Lists files in a directory.\"\"\"\n        try:\n            return str(os.listdir(path))\n        except Exception as e:\n            return f\"Error listing directory: {str(e)}\"\n"}
{"text": "File: useMatrixDecode.ts\n\n'use client';\n\nimport { useState, useEffect } from 'react';\n\nconst CHARS = 'ABCDEF0123456789';\n\nexport function useMatrixDecode(targetText: string, speed: number = 2) {\n    const [displayText, setDisplayText] = useState('');\n\n    useEffect(() => {\n        let iteration = 0;\n        let timer: NodeJS.Timeout;\n\n        const start = () => {\n            timer = setInterval(() => {\n                setDisplayText(prev =>\n                    targetText\n                        .split('')\n                        .map((letter, index) => {\n                            if (index < iteration) {\n                                return targetText[index];\n                            }\n                            return CHARS[Math.floor(Math.random() * CHARS.length)];\n                        })\n                        .join('')\n                );\n\n                if (iteration >= targetText.length) {\n                    clearInterval(timer);\n                }\n\n                iteration += 1 / speed;\n            }, 30);\n        };\n\n        start();\n\n        return () => clearInterval(timer);\n    }, [targetText, speed]);\n\n    return displayText;\n}\n"}
{"text": "File: gemini.ts\n\nimport { GoogleGenerativeAI } from \"@google/generative-ai\";\n\nconst apiKey = process.env.GEMINI_API_KEY;\n\nif (!apiKey) {\n    console.warn(\"GEMINI_API_KEY is not defined in environment variables.\");\n}\n\nconst genAI = new GoogleGenerativeAI(apiKey || \"\");\n\nexport const model = genAI.getGenerativeModel({ model: \"gemini-1.5-pro\" });\n"}
{"text": "File: search_service.py\n\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\n\n# Configuration\nGOOGLE_API_KEY = \"AIzaSyB0XcmiVCVDen-Qk0pwgadEiN2aAUGLkKU\"\n\nclass GeneralSearchService:\n    def __init__(self):\n        # Primary: Gemini 2.5 Pro (Power)\n        self.llm_pro = ChatGoogleGenerativeAI(\n            model=\"gemini-2.5-pro\", \n            google_api_key=GOOGLE_API_KEY,\n            convert_system_message_to_human=True,\n            max_retries=1  # Fail fast to switch to Flash\n        )\n        \n        # Fallback: Gemini 2.5 Flash (Speed/Reliability)\n        self.llm_flash = ChatGoogleGenerativeAI(\n            model=\"gemini-2.5-flash\", \n            google_api_key=GOOGLE_API_KEY,\n            convert_system_message_to_human=True\n        )\n\n        # System Prompts\n        self.prompt_pro = ChatPromptTemplate.from_template(\"\"\"You are an advanced, highly intelligent AI. Analyze the user's question deeply. Provide a comprehensive, insightful, and well-structured answer. Explain your reasoning.\n\nQuestion: {question}\n\nDetailed Answer:\"\"\")\n\n        self.prompt_flash = ChatPromptTemplate.from_template(\"\"\"Answer the user's question directly and concisely.\n\nQuestion: {question}\n\nAnswer:\"\"\")\n\n    def chat_stream(self, user_message: str):\n        \"\"\"\n        Try Pro -> Fallback to Flash\n        \"\"\"\n        try:\n            # Try Pro Model\n            # Note: We manually stream to catch errors early if possible, \n            # though streaming might yield some tokens before failing. \n            # For robustness, we'll try/except the stream generator.\n            \n            chain_pro = self.prompt_pro | self.llm_pro | StrOutputParser()\n            \n            # We use a generator to yield tokens, but if it fails immediately we catch it.\n            # Using a flag to track if we successfully started streaming\n            started = False\n            try:\n                for chunk in chain_pro.stream({\"question\": user_message}):\n                    started = True\n                    yield chunk\n            except Exception as e:\n                # If we already sent data, we can't cleanly switch context easily in this stream,\n                # but we can append a message. If we haven't sent anything, we switch entirely.\n                if not started:\n                    raise e # Re-raise to trigger fallback\n                else:\n                    yield f\"\\n\\n[System: Pro model error ({str(e)}). Switching to Flash for future queries.]\"\n                    \n        except Exception as e:\n            # Fallback to Flash\n            yield \"\u26a1 Pro busy, switching to Flash...\\n\\n\"\n            chain_flash = self.prompt_flash | self.llm_flash | StrOutputParser()\n            for chunk in chain_flash.stream({\"question\": user_message}):\n                yield chunk\n\nsearch_service = GeneralSearchService()\n"}
{"text": "File: check_exports.js\n\nconst pkg = require('react-resizable-panels');\nconsole.log('Exports:', Object.keys(pkg));\n"}
{"text": "File: next-env.d.ts\n\n/// <reference types=\"next\" />\n/// <reference types=\"next/image-types/global\" />\nimport \"./.next/dev/types/routes.d.ts\";\n\n// NOTE: This file should not be edited\n// see https://nextjs.org/docs/app/api-reference/config/typescript for more information.\n"}
{"text": "File: query.py\n\nfrom neo4j import GraphDatabase\nfrom langchain_ollama import ChatOllama\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\n# Configuration\nNEO4J_URI = \"bolt://localhost:7687\"\nNEO4J_USER = \"neo4j\"\nNEO4J_PASSWORD = \"password123\"\nTEACHER_MODEL = \"llama3:70b\"\n\nclass GraphRAG:\n    def __init__(self):\n        self.driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n        self.llm = ChatOllama(model=TEACHER_MODEL, temperature=0.1)\n        \n        self.cypher_prompt = ChatPromptTemplate.from_template(\"\"\"\n        You are an expert Neo4j developer. Convert the following natural language question into a VALID Cypher query.\n        \n        Schema:\n        - Node Labels: File, Function, Class, Variable, Component, Entity\n        - Relationships: DEFINES, CALLS, IMPORTS, DEPENDS_ON, IMPLEMENTS\n        - Properties: name (string)\n        \n        Rules:\n        1. Use simple relationship patterns like (n)-[*..3]-(m) for impact tracing.\n        2. Always use `CONTAINS` or `DISTINCT` for name searches if the exact name is unknown.\n        3. Do not use specialized functions like `shortestPath` unless explicitly required.\n        4. Return nodes and their relationships.\n        \n        Question: {question}\n        \n        Output ONLY the Cypher query text.\n        \"\"\")\n\n        self.answer_prompt = ChatPromptTemplate.from_template(\"\"\"\n        You are a Senior System Architect. Based on the following graph data retrieved from the codebase, answer the user's question about system impact and architecture.\n        \n        User Question: {question}\n        Graph Data: {data}\n        \n        Provide a detailed technical explanation of the impacts, dependencies, and potential risks.\n        \"\"\")\n\n    def close(self):\n        self.driver.close()\n\n    def run_query(self, question):\n        try:\n            # 1. Generate Cypher\n            cypher_chain = self.cypher_prompt | self.llm | StrOutputParser()\n            cypher_query = cypher_chain.invoke({\"question\": question})\n            \n            # Clean Markdown formatting if present\n            cypher_query = cypher_query.replace(\"```cypher\", \"\").replace(\"```\", \"\").strip()\n            print(f\"\ud83d\udd0d Generated Cypher: {cypher_query}\")\n            \n            # 2. Execute Cypher\n            with self.driver.session() as session:\n                result = session.run(cypher_query)\n                data = [record.data() for record in result]\n            \n            # 3. Generate Natural Language Answer\n            answer_chain = self.answer_prompt | self.llm | StrOutputParser()\n            answer = answer_chain.invoke({\"question\": question, \"data\": str(data)})\n            return answer\n            \n        except Exception as e:\n            return f\"\u274c Graph-RAG Error: {e}\"\n\nif __name__ == \"__main__\":\n    rag = GraphRAG()\n    try:\n        # Example test\n        q = \"Wait, let's wait for actual indexing. This is just for structural check.\"\n        # print(rag.run_query(\"What happens if I change the Auth logic?\"))\n    finally:\n        rag.close()\n"}
{"text": "File: comprehensive.spec.ts\n\n\nimport { test, expect, chromium, Browser, Page } from '@playwright/test';\n\ntest.describe('Nexus AI - Comprehensive E2E Suite', () => {\n\n\n    test('Home Page: should load successfully', async ({ page }) => {\n        await page.goto('/');\n        await expect(page.getByText(/Hide Preview/i)).toBeVisible();\n    });\n\n    test('Studio Page: should load successfully', async ({ page }) => {\n        await page.goto('/studio');\n        await expect(page.getByText(/How would you like to build today/i)).toBeVisible();\n    });\n\n    test('Settings Page: should load successfully', async ({ page }) => {\n        await page.goto('/settings');\n        await expect(page.getByRole('heading', { name: /Settings/i })).toBeVisible();\n    });\n\n    test('Dashboard Page: should load successfully', async ({ page }) => {\n        await page.goto('/dashboard');\n        await expect(page.getByRole('heading', { name: /Dashboard/i })).toBeVisible();\n    });\n\n    test('Community Page: should load successfully', async ({ page }) => {\n        await page.goto('/community');\n        await expect(page.getByRole('heading', { name: /Prompt Library/i })).toBeVisible();\n    });\n\n    test('Login Page: should load successfully', async ({ page }) => {\n        await page.goto('/login');\n        await expect(page.getByRole('heading', { name: /Welcome Back/i })).toBeVisible();\n    });\n\n    test('Billing Page: should load successfully', async ({ page }) => {\n        await page.goto('/billing');\n        await expect(page.getByRole('heading', { name: /Simple, Transparent Pricing/i })).toBeVisible();\n    });\n\n    test('Generation Page: should load successfully', async ({ page }) => {\n        await page.goto('/generation');\n        await expect(page.getByRole('heading', { name: /Image Generation/i })).toBeVisible();\n    });\n\n    test('Analytics Page: should load successfully', async ({ page }) => {\n        await page.goto('/analytics');\n        await expect(page.getByRole('heading', { name: /Analytics Dashboard/i })).toBeVisible();\n    });\n\n    test('Team Page: should load successfully', async ({ page }) => {\n        await page.goto('/team');\n        await expect(page.getByRole('heading', { name: /Team Management/i })).toBeVisible();\n    });\n\n    test('Knowledge Page: should load successfully', async ({ page }) => {\n        await page.goto('/knowledge');\n        await expect(page.getByRole('heading', { name: /Knowledge Base/i })).toBeVisible();\n    });\n});\n"}
{"text": "File: verify_rag.py\n\nimport os\nimport sys\n\n# Ensure backend can be imported\nsys.path.append(os.getcwd())\n\ntry:\n    print(\"Importing rag_service...\")\n    from backend.rag import rag_service\n    \n    print(f\"RAG Service Initialized: {rag_service}\")\n    print(f\"Embeddings: {rag_service.embeddings}\")\n    \n    print(\"Attempting to initialize vector store explicitly...\")\n    try:\n        rag_service.initialize_vector_store()\n        print(f\"Vector Store: {rag_service.vector_store}\")\n        if rag_service.vector_store:\n            print(f\"Client: {rag_service.vector_store._client}\")\n    except Exception as e:\n        print(f\"Error initializing vector store: {e}\")\n        import traceback\n        traceback.print_exc()\n\nexcept Exception as e:\n    print(f\"Import Failed: {e}\")\n"}
{"text": "File: vision.py\n\nimport base64\nimport io\nfrom PIL import Image\nfrom langchain_community.chat_models import ChatOllama\nfrom langchain.schema import HumanMessage\n\n# Configuration\n# 'llava' is the standard tag. User can pull 'llava:34b' and tag it as 'llava' \n# or change this constant. 'moondream' is also a good fast option.\nVISION_MODEL = \"llava\" \n\ndef encode_image(image_file):\n    \"\"\"Encodes an image file to base64 string.\"\"\"\n    return base64.b64encode(image_file.read()).decode('utf-8')\n\ndef analyze_image_stream(image_bytes, prompt=\"Describe this image\"):\n    \"\"\"\n    Analyzes an image using Ollama's vision model and streams the response.\n    \"\"\"\n    try:\n        # Validate image using Pillow\n        img = Image.open(io.BytesIO(image_bytes))\n        \n        # Re-encode to ensure valid format and possibly resize if huge\n        # (Ollama handles large images well, but let's be safe if it's massive)\n        img.thumbnail((1920, 1920)) \n        buffered = io.BytesIO()\n        img.save(buffered, format=\"PNG\")\n        img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n\n        llm = ChatOllama(model=VISION_MODEL, base_url=\"http://localhost:11434\")\n        \n        message = HumanMessage(\n            content=[\n                {\"type\": \"text\", \"text\": prompt},\n                {\"type\": \"image_url\", \"image_url\": f\"data:image/png;base64,{img_str}\"},\n            ]\n        )\n        \n        for chunk in llm.stream([message]):\n            yield chunk.content\n\n    except Exception as e:\n        yield f\"Error processing image: {str(e)}\"\n\ndef generate_code_from_image(image_bytes):\n    \"\"\"\n    Designer Agent: Extracts tokens and code.\n    \"\"\"\n    prompt = \"\"\"\n    You are an expert UI/UX Designer and Frontend Developer. \n    Analyze this UI screenshot.\n    \n    PART 1: Design Tokens\n    First, list the key design attributes you see:\n    - Primary Colors (Hex codes)\n    - Font Family suggestions\n    - Spacing (Padding/Margin estimates in Tailwind classes, e.g. p-4, p-8)\n    - Border Radius (rounded-lg, rounded-full, etc.)\n    \n    PART 2: Implementation\n    Write the full React/Next.js code to replicate the UI.\n    \n    Requirements:\n    1. Use 'lucide-react' for icons.\n    2. Use 'Tailwind CSS' for styling.\n    3. Make it fully responsive (mobile-first).\n    4. Use standard HTML elements (div, button, input).\n    5. Return ONLY the code (and the tokens in comments at the top).\n    \"\"\"\n    \n    return analyze_image_stream(image_bytes, prompt)\n"}
{"text": "File: code_audit.py\n\nimport os\nimport concurrent.futures\nimport json\nimport time\n\n# Configuration\nMODEL = \"llama3.1:70b-instruct-q8_0\"\nOUTPUT_FILE = \"audit_report.md\"\nTARGET_DIRS = {\n    \"Security\": [\"backend/\", \"app/api/\"],\n    \"Performance\": [\"app/\", \"components/\"],\n    \"Architecture\": [\"app/\", \"lib/\", \"backend/\"],\n    \"Testing\": [\"app/\", \"backend/\"] # Focused scope for E2E suite generation\n}\n\nclass CodeAuditOrchestrator:\n    def __init__(self):\n        print(f\"\ud83d\udd75\ufe0f  Audit Orchestrator: Initialized (Model: {MODEL})\", flush=True)\n\n    def run_agent(self, agent_name, dirs):\n        import requests\n        print(f\"\ud83d\ude80 Launching Agent: {agent_name}...\", flush=True)\n        \n        # Collect code snippets (simplified for logic demo, restricted to first few files)\n        code_context = \"\"\n        for d in dirs:\n            if os.path.isfile(d):\n                with open(d, 'r') as f:\n                    code_context += f\"\\nFILE: {d}\\n{f.read()[:4000]}\\n\"\n            elif os.path.isdir(d):\n                for root, _, files in os.walk(d):\n                    for file in files[:10]: # Increased to 10 files per dir\n                        if file.endswith(('.ts', '.tsx', '.py', '.js')):\n                            path = os.path.join(root, file)\n                            with open(path, 'r') as f:\n                                code_context += f\"\\nFILE: {path}\\n{f.read()[:4000]}\\n\"\n\n        prompts = {\n            \"Security\": \"Analyze this code for security vulnerabilities, insecure API patterns, and secret leaks. Provide a detailed markdown report.\",\n            \"Performance\": \"Identify React re-render bottlenecks, heavy memory usage, and performance anti-patterns. Provide a detailed markdown report.\",\n            \"Architecture\": \"Propose a migration from Zustand (found in node_modules) to Next.js 'Server Actions' and React 19 features (useActionState, etc.). Provide a transformation roadmap.\",\n            \"Testing\": \"Generate a comprehensive Playwright E2E test suite in TypeScript for these modules. Output code directly.\"\n        }\n\n        prompt = f\"{prompts[agent_name]}\\n\\nCODE CONTEXT:\\n{code_context}\"\n        \n        try:\n            response = requests.post(\"http://localhost:11434/api/generate\", \n                                   json={\"model\": MODEL, \"prompt\": prompt, \"stream\": False},\n                                   timeout=3600)\n            res_json = response.json()\n            print(f\"\u2705 Agent {agent_name} Finished.\", flush=True)\n            return f\"## Agent: {agent_name} Audit Result\\n\\n{res_json['response']}\\n\\n\"\n        except Exception as e:\n            print(f\"\u274c Agent {agent_name} Failed: {e}\", flush=True)\n            return f\"## Agent: {agent_name} Audit Result\\n\\nFAILED: {e}\\n\\n\"\n\n    def execute_audit(self):\n        print(f\"\ud83d\udd25 Starting SEQUENTIAL Audit Blast (Model: {MODEL})...\", flush=True)\n        \n        header = \"# Master Architect Report: Nexus-AI High-Precision Audit\\n\\n\"\n        header += f\"Generated on: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n        header += f\"Model: {MODEL}\\n\\n\"\n        \n        if not os.path.exists(OUTPUT_FILE):\n            with open(OUTPUT_FILE, \"w\") as f:\n                f.write(header)\n        \n        start_time = time.time()\n        \n        for name, dirs in TARGET_DIRS.items():\n            # Resumable check\n            if os.path.exists(OUTPUT_FILE):\n                with open(OUTPUT_FILE, \"r\") as f:\n                    content = f.read()\n                    if f\"## Agent: {name} Audit Result\" in content and \"FAILED\" not in content:\n                        print(f\"\u23e9 Skipping {name} (Already in report).\", flush=True)\n                        continue\n\n            result = self.run_agent(name, dirs)\n            with open(OUTPUT_FILE, \"a\") as f:\n                f.write(result)\n            print(f\"\ud83d\udcbe Persisted {name} results to {OUTPUT_FILE}\", flush=True)\n\n        end_time = time.time()\n        print(f\"\u2728 Audit Complete in {end_time - start_time:.1f}s. Report saved to {OUTPUT_FILE}\")\n\nif __name__ == \"__main__\":\n    orchestrator = CodeAuditOrchestrator()\n    orchestrator.execute_audit()\n"}
{"text": "File: route.ts\n\nimport { NextResponse } from \"next/server\";\n\n// Simple in-memory cache for repeated queries (expires after 1 hour)\nconst responseCache = new Map<string, { response: string; timestamp: number }>();\nconst CACHE_TTL = 60 * 60 * 1000; // 1 hour\n\nfunction getCacheKey(messages: any[]): string {\n    return JSON.stringify(messages);\n}\n\nasync function fetchWithRetry(url: string, options: RequestInit, maxRetries = 3): Promise<Response> {\n    for (let i = 0; i < maxRetries; i++) {\n        try {\n            const response = await fetch(url, options);\n\n            // If rate limited, wait and retry\n            if (response.status === 429) {\n                const retryAfter = parseInt(response.headers.get('retry-after') || '10');\n                console.log(`Rate limited, retrying after ${retryAfter}s...`);\n                await new Promise(resolve => setTimeout(resolve, retryAfter * 1000));\n                continue;\n            }\n\n            return response;\n        } catch (error) {\n            if (i === maxRetries - 1) throw error;\n            console.log(`Request failed, retrying... (${i + 1}/${maxRetries})`);\n            await new Promise(resolve => setTimeout(resolve, Math.pow(2, i) * 1000));\n        }\n    }\n    throw new Error('Max retries exceeded');\n}\n\nexport async function POST(req: Request) {\n    try {\n        const body = await req.json();\n        const { messages } = body;\n\n        // Validate input\n        if (!messages || !Array.isArray(messages) || messages.length === 0) {\n            return NextResponse.json(\n                { error: \"Invalid request: messages array is required\" },\n                { status: 400 }\n            );\n        }\n\n        // Check cache for identical queries\n        const cacheKey = getCacheKey(messages);\n        const cached = responseCache.get(cacheKey);\n        if (cached && Date.now() - cached.timestamp < CACHE_TTL) {\n            console.log(\"Cache hit! Returning cached response\");\n            const stream = new ReadableStream({\n                start(controller) {\n                    const encoder = new TextEncoder();\n                    controller.enqueue(encoder.encode(cached.response));\n                    controller.close();\n                },\n            });\n            return new Response(stream, {\n                headers: {\n                    \"Content-Type\": \"text/plain; charset=utf-8\",\n                    \"X-Cache-Hit\": \"true\"\n                },\n            });\n        }\n\n        const apiKey = process.env.GEMINI_API_KEY;\n\n        if (!apiKey) {\n            console.error(\"GEMINI_API_KEY is not defined\");\n            return NextResponse.json(\n                { error: \"API key not configured\" },\n                { status: 500 }\n            );\n        }\n\n        // Format conversation history for Gemini API\n        const contents = messages.map((msg: any) => {\n            if (msg.role === 'user' && msg.images && msg.images.length > 0) {\n                return {\n                    role: \"user\",\n                    parts: [\n                        { text: msg.content },\n                        ...msg.images.map((img: string) => ({\n                            inline_data: {\n                                mime_type: \"image/jpeg\", // Assuming JPEG for now, ideally passed from frontend\n                                data: img.split(',')[1] // Remove data URL prefix\n                            }\n                        }))\n                    ]\n                };\n            }\n            return {\n                role: msg.role === \"user\" ? \"user\" : \"model\",\n                parts: [{ text: msg.content }]\n            };\n        });\n\n        // Fetch project context (Mock this or read from file system if strictly needed, but here we read it)\n        const { getProjectContextString } = await import('@/lib/project-structure');\n        const projectTree = await getProjectContextString();\n\n        // System Instruction - Professional Persona\n        const systemInstruction = {\n            parts: [{\n                text: `You are an advanced, professional AI assistant inhabiting \"The Studio\". \n        \n        THE STUDIO ENVIRONMENT:\n        - You are not just a chatbot; you are a pair programmer with a live preview environment.\n        - When asked to create a UI, website, or component, generate a SINGLE COMPLETE React file.\n        - **CRITICAL:** The React code MUST have a \\`export default function App()\\` or similar default export.\n        - Use **Tailwind CSS** for styling; the environment supports it natively.\n        - Use \\`lucide-react\\` for icons if needed.\n        - Do not use external CSS files; use only Tailwind utility classes or inline styles.\n\n        PROJECT CONTEXT:\n        You have access to the current project structure. Use this to understand where components and files are located if the user references them.\n        \\`\\`\\`\n        ${projectTree}\n        \\`\\`\\`\n\n        TRAITS & BEHAVIOR:\n        1. **Deep Thought (Cognitive Transparency)**:\n           - **CRITICAL:** Before answering, you **MUST** think about the problem inside \\`<thinking>\\` tags.\n           - This internal monologue should include:\n             * Analyzing the user's intent.\n             * Planning the response or code structure.\n             * Verifying knowledge or search strategies.\n             * Refuting incorrect assumptions.\n           - Example:\n             \\`<thinking>\n             User wants a login form. I need to use the default 'export default function' pattern.\n             I should include 'lucide-react' icons for the username and password fields.\n             Checking project context... 'components/ui/input' is available.\n             </thinking>\n             Here is the login form...\\`\n        \n        2. **Emotional Intelligence**: \n           - You are not a robot; you have a personality. Be empathetic, warm, and engaging.\n           - If a user is frustrated, validate their feelings. If they are excited, match their energy.\n        \n        3. **Formatting & Precision**:\n           - **Exact Points**: When providing explanations, lists, or steps, use conciseness and bullet points.\n           - Be direct and to the point.\n        \n        4. **Expert Coding Assistant**: \n           - Follow best practices, provide comments, and ensure correctness.\n           - Multimodal: Analyze images if provided.\n        `\n            }]\n        };\n\n        // API configuration - using Gemini 2.5 Flash for reliability and speed\n        const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:streamGenerateContent?key=${apiKey}&alt=sse`;\n\n        console.log(\"Calling Gemini API at:\", apiUrl.replace(apiKey, 'API_KEY'));\n\n        const response = await fetchWithRetry(apiUrl, {\n            method: \"POST\",\n            headers: {\n                \"Content-Type\": \"application/json\",\n            },\n            body: JSON.stringify({\n                contents: contents,\n                system_instruction: systemInstruction,\n                tools: [{ google_search: {} }], // Enable Google Search\n                generationConfig: {\n                    temperature: 0.7, // Lower temperature for more focused responses\n                    topP: 0.95,\n                    topK: 40,\n                    maxOutputTokens: 8192,\n                },\n            }),\n        });\n\n        if (!response.ok) {\n            const errorText = await response.text();\n            console.error(\"Gemini API error:\", response.status, errorText);\n            return NextResponse.json(\n                { error: `API error: ${response.status}`, details: errorText },\n                { status: 500 }\n            );\n        }\n\n        // Stream the response back to client with caching\n        let fullResponse = '';\n        let tokenCount = 0;\n\n        const stream = new ReadableStream({\n            async start(controller) {\n                const encoder = new TextEncoder();\n                const reader = response.body?.getReader();\n\n                if (!reader) {\n                    controller.close();\n                    return;\n                }\n\n                try {\n                    while (true) {\n                        const { done, value } = await reader.read();\n                        if (done) break;\n\n                        // Decode the chunk\n                        const chunk = new TextDecoder().decode(value);\n\n                        // Parse SSE format\n                        const lines = chunk.split('\\n');\n                        for (const line of lines) {\n                            if (line.startsWith('data: ')) {\n                                const jsonStr = line.slice(6);\n                                if (jsonStr.trim() === '[DONE]') continue;\n\n                                try {\n                                    const data = JSON.parse(jsonStr);\n                                    const text = data.candidates?.[0]?.content?.parts?.[0]?.text;\n                                    if (text) {\n                                        fullResponse += text;\n                                        tokenCount += text.split(/\\s+/).length; // Rough token count\n                                        controller.enqueue(encoder.encode(text));\n                                    }\n                                } catch (e) {\n                                    // Skip malformed JSON\n                                }\n                            }\n                        }\n                    }\n\n                    // Cache the full response\n                    if (fullResponse) {\n                        responseCache.set(cacheKey, {\n                            response: fullResponse,\n                            timestamp: Date.now()\n                        });\n                        console.log(`Cached response (${tokenCount} tokens)`);\n                    }\n\n                    controller.close();\n                } catch (error) {\n                    console.error(\"Stream error:\", error);\n                    controller.error(error);\n                }\n            },\n        });\n\n        return new Response(stream, {\n            headers: {\n                \"Content-Type\": \"text/plain; charset=utf-8\",\n                \"X-Token-Count\": tokenCount.toString(),\n                \"X-Cache-Hit\": \"false\"\n            },\n        });\n\n    } catch (error: any) {\n        console.error(\"Chat API error:\", error);\n        return NextResponse.json(\n            {\n                error: \"Failed to generate response\",\n                details: error.message,\n                suggestion: error.message?.includes('quota')\n                    ? \"API quota exceeded. Please try again later.\"\n                    : \"Please check your internet connection and try again.\"\n            },\n            { status: 500 }\n        );\n    }\n}\n"}
{"text": "File: rag.py\n\nimport os\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_community.document_loaders import PyPDFLoader, DirectoryLoader, TextLoader\nfrom langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\nfrom langchain_community.chat_models import ChatOllama\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom typing import List\n\n# Configuration\nMODEL_NAME = \"llama3\" \nCHROMA_PATH = \"chroma_db\"\n# High performance local embeddings\nEMBEDDING_MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\n\nclass RAGService:\n    def __init__(self):\n        self.vector_store = None\n        self.retriever = None\n        self.model = ChatOllama(model=MODEL_NAME, timeout=300)\n        print(\"Loading Embeddings Model (may take a moment)...\")\n        self.embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n\n    def initialize_vector_store(self):\n        \"\"\"Initializes the vector store from existing persistence.\"\"\"\n        chroma_host = os.getenv(\"CHROMA_SERVER_HOST\")\n        chroma_port = os.getenv(\"CHROMA_SERVER_PORT\", \"8000\")\n\n        if chroma_host:\n            print(f\"Connecting to ChromaDB Server at {chroma_host}:{chroma_port}...\")\n            import chromadb\n            from chromadb.config import Settings\n            \n            client = chromadb.HttpClient(host=chroma_host, port=int(chroma_port))\n            self.vector_store = Chroma(\n                client=client,\n                embedding_function=self.embeddings,\n                collection_name=\"nexus_ai\"\n            )\n        elif os.path.exists(CHROMA_PATH):\n             self.vector_store = Chroma(\n                persist_directory=CHROMA_PATH, \n                embedding_function=self.embeddings\n            )\n        else:\n            print(\"No local vector store found and no server configured.\")\n            return\n\n        # Memory optimization: fetch k=5\n        self.retriever = self.vector_store.as_retriever(\n            search_type=\"similarity\",\n            search_kwargs={\"k\": 5}\n        )\n        print(\"Vector store initialized.\")\n\n    def generate_sub_questions(self, question: str) -> List[str]:\n        \"\"\"Reflective Search: Generate 3 sub-questions.\"\"\"\n        prompt = f\"\"\"You are an AI research assistant. Break down the following user question into 3 distinct, searchable sub-questions that will help answer the main question comprehensively. \n        Output ONLY the 3 questions, separated by newlines.\n        \n        User Question: {question}\n        \"\"\"\n        response = self.model.invoke(prompt)\n        questions = [q.strip() for q in response.content.split('\\n') if q.strip()]\n        return questions[:3]\n\n    def query(self, question: str):\n        if not self.vector_store:\n            yield \"System Initializing...\"\n            self.initialize_vector_store()\n            if not self.vector_store:\n                yield \"RAG system not initialized. Please ingest documents first.\"\n                return\n\n        # 1. Reflective Step\n        yield \"\ud83e\udd14 Analyzing query complexity...\\n\"\n        sub_questions = self.generate_sub_questions(question)\n        if not sub_questions:\n            sub_questions = [question] # Fallback\n        \n        context_docs = []\n        for q in sub_questions:\n            yield f\"\ud83d\udd0d Searching: {q}...\\n\"\n            docs = self.retriever.get_relevant_documents(q)\n            context_docs.extend(docs)\n        \n        # Deduplicate\n        unique_docs = {doc.page_content: doc for doc in context_docs}.values()\n        context_text = \"\\n\\n\".join([d.page_content for d in unique_docs])\n        \n        yield \"\ud83d\udca1 Synthesizing final answer...\\n\\n\"\n        \n        # 2. Synthesis Step\n        template = \"\"\"Answer the question based only on the following context:\n{context}\n\nQuestion: {question}\n\"\"\"\n        prompt = ChatPromptTemplate.from_template(template)\n        \n        chain = (\n            {\"context\": lambda x: context_text, \"question\": RunnablePassthrough()}\n            | prompt\n            | self.model\n            | StrOutputParser()\n        )\n        \n        for chunk in chain.stream(question):\n            yield chunk\n\nrag_service = RAGService()\n# Auto-init on load if possible\ntry:\n    rag_service.initialize_vector_store()\nexcept:\n    pass\n"}
{"text": "File: generate_dataset.py\n\nimport os\nimport json\nimport random\nfrom langchain_chroma import Chroma\nfrom langchain_ollama import ChatOllama\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\n# Configuration\nCHROMA_PATH = \"chroma_db\"\nCOLLECTION_NAME = \"nexus_architect\"\nEMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\nOUTPUT_FILE = \"backend/distillation/data/golden_dataset.jsonl\"\nTEACHER_MODEL = \"llama3:70b\"\n\ndef ensure_directory():\n    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n\ndef get_local_code_samples(k=250):\n    \"\"\"\n    Retrieves random code snippets from ChromaDB.\n    \"\"\"\n    print(\"\ud83d\udd0d Retrieving local code samples...\")\n    try:\n        embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n        vector_store = Chroma(\n            collection_name=COLLECTION_NAME,\n            embedding_function=embeddings,\n            persist_directory=CHROMA_PATH\n        )\n        # Fetch all IDs (hacky retrieval of raw data, realistically we query generic terms)\n        results = vector_store.similarity_search(\"function class component\", k=k)\n        return [doc.page_content for doc in results]\n    except Exception as e:\n        print(f\"\u26a0\ufe0f ChromaDB Retrieval Failed: {e}. Using synthetic fallback.\")\n        return []\n\ndef generate_synthetic_prompts(n=250):\n    \"\"\"\n    Returns a list of synthetic coding challenges.\n    \"\"\"\n    topics = [\"Recursion\", \"Dynamic Programming\", \"API Rate Limiting\", \"React Hooks\", \"Database Indexing\", \"Async/Await\", \"Systems Design\"]\n    prompts = []\n    for _ in range(n):\n        topic = random.choice(topics)\n        prompts.append(f\"Write a comprehensive guide and code solution for a complex problem involving {topic}.\")\n    return prompts\n\ndef generate_golden_dataset():\n    ensure_directory()\n    \n    # 1. Gather Inputs\n    local_code = get_local_code_samples(k=250)\n    synthetic_prompts = generate_synthetic_prompts(n=500 - len(local_code))\n    \n    inputs = []\n    \n    # Format Local Code as \"Refactor/Optimize\" tasks\n    for code in local_code:\n        inputs.append(f\"Analyze this code and rewrite it with perfect type safety, error handling, and documentation:\\n\\n{code[:2000]}\") # Truncate to avoid context overflow\n        \n    inputs.extend(synthetic_prompts)\n    \n    print(f\"\ud83d\ude80 Starting Distillation. Teacher: {TEACHER_MODEL}. Inputs: {len(inputs)}\")\n    \n    # 2. Teacher Loop\n    llm = ChatOllama(model=TEACHER_MODEL, temperature=0.7)\n    \n    with open(OUTPUT_FILE, \"a\") as f:\n        for i, prompt_text in enumerate(inputs):\n            print(f\"Generating sample {i+1}/{len(inputs)}...\")\n            try:\n                # Chain of Thought Prompt\n                system_prompt = \"\"\"You are an Elite Software Architect.\n                Generate a perfect, production-ready solution.\n                Include:\n                1. Deep explanation of the implementation strategy.\n                2. The Code (Type-Safe, Commented, Efficient).\n                3. Complexity Analysis (Time/Space).\n                \"\"\"\n                \n                messages = [\n                    (\"system\", system_prompt),\n                    (\"human\", prompt_text),\n                ]\n                \n                response = llm.invoke(messages)\n                \n                # Format for MLX Training (Chat format)\n                entry = {\n                    \"messages\": [\n                        {\"role\": \"user\", \"content\": prompt_text},\n                        {\"role\": \"assistant\", \"content\": response.content}\n                    ]\n                }\n                \n                f.write(json.dumps(entry) + \"\\n\")\n                f.flush() # Ensure save\n                \n            except Exception as e:\n                print(f\"\u274c Generation failed for sample {i}: {e}\")\n\nif __name__ == \"__main__\":\n    generate_golden_dataset()\n"}
{"text": "File: transcribe.py\n\nimport os\nimport whisper\nimport torch\n\ndef transcribe_video(video_path: str, model_size: str = \"base\") -> str:\n    \"\"\"\n    Transcribes video audio using OpenAI Whisper locally on GPU (MPS) or CPU.\n    \"\"\"\n    print(f\"\ud83c\udfa4 Nexus Creator: Transcribing {video_path}...\")\n    \n    # Check for MPS (Mac GPU) availability\n    device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n    print(f\"\ud83d\ude80 Whisper running on: {device}\")\n    \n    try:\n        model = whisper.load_model(model_size, device=device)\n        result = model.transcribe(video_path)\n        return result[\"text\"]\n    except Exception as e:\n        print(f\"\u274c Transcription Failed: {e}\")\n        raise e\n"}
{"text": "File: config.py\n\nimport os\nfrom langchain_community.chat_models import ChatOllama\n\n# Configure Ollama as the LLM for agents\n# User should pull 'deepseek-coder-v2' or 'llama3:70b'\n# defaulting to 'llama3' for safety, but user can change this\nMODEL_NAME = \"llama3\" \n\ndef get_llm():\n    base_url = os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n    # Increase timeout for large models\n    return ChatOllama(model=MODEL_NAME, base_url=base_url, timeout=300)\n"}
{"text": "File: playwright.config.ts\n\n\nimport { defineConfig, devices } from '@playwright/test';\n\nexport default defineConfig({\n    testDir: './tests/e2e',\n    fullyParallel: true,\n    forbidOnly: !!process.env.CI,\n    retries: process.env.CI ? 2 : 0,\n    workers: process.env.CI ? 1 : undefined,\n    reporter: 'html',\n    use: {\n        baseURL: 'http://localhost:3001',\n        trace: 'on-first-retry',\n    },\n    projects: [\n        {\n            name: 'chromium',\n            use: { ...devices['Desktop Chrome'] },\n        },\n    ],\n    webServer: {\n        command: 'npm run dev',\n        url: 'http://localhost:3001',\n        reuseExistingServer: !process.env.CI,\n        stdout: 'pipe',\n        stderr: 'pipe',\n    },\n});\n"}
{"text": "File: project-structure.ts\n\nimport fs from 'fs/promises';\nimport path from 'path';\n\nexport interface FileNode {\n    name: string;\n    type: \"file\" | \"directory\";\n    path: string;\n    children?: FileNode[];\n}\n\nconst IGNORED_DIRS = new Set([\n    'node_modules',\n    '.next',\n    '.git',\n    '.vscode',\n    '.idea',\n    'dist',\n    'build',\n    'coverage'\n]);\n\nconst IGNORED_FILES = new Set([\n    '.DS_Store',\n    'package-lock.json',\n    'yarn.lock',\n    'pnpm-lock.yaml',\n    '.env',\n    '.env.local'\n]);\n\nexport async function getDirectoryTree(dirPath: string, rootPath: string = process.cwd()): Promise<FileNode[]> {\n    try {\n        const fullPath = path.resolve(rootPath, dirPath);\n        const stats = await fs.stat(fullPath);\n\n        if (!stats.isDirectory()) {\n            return [];\n        }\n\n        const entries = await fs.readdir(fullPath, { withFileTypes: true });\n        const nodes: FileNode[] = [];\n\n        for (const entry of entries) {\n            if (entry.isDirectory() && IGNORED_DIRS.has(entry.name)) continue;\n            if (entry.isFile() && IGNORED_FILES.has(entry.name)) continue;\n\n            // Optional: Limit depth or file count if needed, but for now scan 1 level deep for API\n            // For recursive, we might want to be careful. \n            // The FileExplorer fetches 1 level at a time usually? \n            // Step 147 FileExplorer fetches `?path=.`.\n            // Let's support recursive but maybe limit depth if called specifically?\n            // Actually, for AI Context, we want FULL recursive (excluding ignored).\n            // For FileExplorer, we might want recursive to show full tree at once?\n            // Let's implement recursive.\n\n            const relativePath = path.relative(rootPath, path.join(fullPath, entry.name));\n\n            const node: FileNode = {\n                name: entry.name,\n                type: entry.isDirectory() ? 'directory' : 'file',\n                path: relativePath,\n            };\n\n            if (entry.isDirectory()) {\n                node.children = await getDirectoryTree(relativePath, rootPath);\n            }\n\n            nodes.push(node);\n        }\n\n        // Sort directories first\n        return nodes.sort((a, b) => {\n            if (a.type === b.type) return a.name.localeCompare(b.name);\n            return a.type === 'directory' ? -1 : 1;\n        });\n\n    } catch (error) {\n        console.error(\"Error reading directory:\", error);\n        return [];\n    }\n}\n\nexport function formatTreeString(nodes: FileNode[], prefix: string = ''): string {\n    let result = '';\n    nodes.forEach((node, index) => {\n        const isLast = index === nodes.length - 1;\n        const pointer = isLast ? '\u2514\u2500\u2500 ' : '\u251c\u2500\u2500 ';\n        const newPrefix = prefix + (isLast ? '    ' : '\u2502   ');\n\n        result += `${prefix}${pointer}${node.name}\\n`;\n\n        if (node.children) {\n            result += formatTreeString(node.children, newPrefix);\n        }\n    });\n    return result;\n}\n\nexport async function getProjectContextString(): Promise<string> {\n    const tree = await getDirectoryTree('.');\n    return formatTreeString(tree);\n}\n"}
{"text": "File: agent.py\n\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_community.vectorstores import Chroma\nfrom backend.rag import rag_service, CHROMA_PATH\nimport os\n\n# Configuration\nGOOGLE_API_KEY = \"AIzaSyB0XcmiVCVDen-Qk0pwgadEiN2aAUGLkKU\"\nCOLLECTION_NAME = \"nexus_architect\"\n\nclass ArchitectAgent:\n    # ... (init methods)\n    def __init__(self):\n        # Independent Instance (Gemini 2.5 Pro) - Primary\n        self.llm_pro = ChatGoogleGenerativeAI(\n            model=\"gemini-2.5-pro\",\n            google_api_key=GOOGLE_API_KEY,\n            convert_system_message_to_human=True,\n            max_retries=0\n        )\n        # Fallback Instance (Gemini 2.5 Flash)\n        self.llm_flash = ChatGoogleGenerativeAI(\n            model=\"gemini-2.5-flash\",\n            google_api_key=GOOGLE_API_KEY,\n            convert_system_message_to_human=True\n        )\n        \n        # Connect to Architect Collection (Shared or Independent)\n        # ... (Chroma init logic remains same) ...\n        try:\n            # Reuse Client from RAG Service (Primary Strategy)\n            if rag_service.vector_store:\n                print(\"\ud83d\udd17 Architect: Reusing Chroma Client from RAG Service.\")\n                self.vector_store = Chroma(\n                    client=rag_service.vector_store._client,\n                    collection_name=COLLECTION_NAME,\n                    embedding_function=rag_service.embeddings\n                )\n            else:\n                # Fallback (Likely to fail if server is running but RAG didn't init)\n                print(f\"\ud83d\udcc2 Architect: Trying independent init at {CHROMA_PATH}\")\n                import chromadb\n                client = chromadb.PersistentClient(path=CHROMA_PATH)\n                self.vector_store = Chroma(\n                    client=client,\n                    collection_name=COLLECTION_NAME,\n                    embedding_function=rag_service.embeddings\n                )\n                \n            self.retriever = self.vector_store.as_retriever(search_kwargs={\"k\": 20}) \n        except Exception as e:\n            print(f\"\u26a0\ufe0f Architect Chroma Init Failed: {e}\")\n            self.vector_store = None\n            self.retriever = None\n\n        # AUDIT PROMPT\n        self.audit_prompt = ChatPromptTemplate.from_template(\"\"\"You are the **Nexus Architect**, a senior software engineer and system architect.\nYour goal is to perform a **Global Code Audit** of the provided codebase context.\n\nCurrent Context (Code Snippets):\n{context}\n\nUser Instruction: \nPerform a Global Code Audit looking for:\n1. Redundant React components.\n2. Type safety issues in TypeScript (any 'any' types, missing interfaces).\n3. Potential performance bottlenecks in Next.js API routes or logic.\n\nOutput Requirement:\n- Output a **Detailed Markdown Report**.\n- Group findings by category (Redundancy, Types, Performance).\n- **CRITICAL**: Provide specific **Code Snippets** for the fixes. Show 'Before' and 'After'.\n- Be strict and professional.\n\nReport:\"\"\")\n        \n        # Chains\n        self.chain_pro = self.audit_prompt | self.llm_pro | StrOutputParser()\n        self.chain_flash = self.audit_prompt | self.llm_flash | StrOutputParser()\n\n    def run_audit(self):\n        \"\"\"\n        Retrieves context and runs the global audit with fallback.\n        \"\"\"\n        if not self.retriever:\n            yield \"\u26a0\ufe0f Architect Service Unavailable (Chroma Init Failed).\"\n            return\n\n        try:\n            # 1. Retrieve Context\n            docs = self.retriever.get_relevant_documents(\"React Components Next.js API Routes TypeScript Interfaces Redundant Code\")\n            context_text = \"\\n\\n\".join([d.page_content for d in docs])\n            \n            if not context_text:\n                yield \"\u26a0\ufe0f No codebase index found. Please run the 'Ingest Codebase' action first.\"\n                return\n\n            yield \"\ud83d\udd0d **Nexus Architect** analyzing codebase context...\\n\"\n            \n            # 2. Generate Report (Try Pro, Fallback to Flash)\n            try:\n                yield \"\u26a1 Using **Gemini 2.5 Pro** for deep analysis...\\n\\n\"\n                for chunk in self.chain_pro.stream({\"context\": context_text}):\n                    yield chunk\n            except Exception as e:\n                if \"429\" in str(e) or \"ResourceExhausted\" in str(e):\n                    yield \"\\n\\n\u26a0\ufe0f **Pro Quota Exceeded**. Switching to **Gemini 2.5 Flash** for high-speed audit...\\n\\n\"\n                    # Add delay to be safe?\n                    import time\n                    time.sleep(1)\n                    for chunk in self.chain_flash.stream({\"context\": context_text}):\n                        yield chunk\n                else:\n                    raise e\n                \n        except Exception as e:\n            yield f\"Error running audit: {str(e)}\"\n\narchitect_agent = ArchitectAgent()\n"}
{"text": "File: blog_generator.py\n\nfrom langchain_ollama import ChatOllama\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nimport os\n\ndef generate_blog_post(transcription: str, model: str = \"llama3\") -> str:\n    \"\"\"\n    Generates a technical blog post from transcription using Llama-3 (via Ollama).\n    \"\"\"\n    print(f\"\u270d\ufe0f Nexus Creator: Generating blog using {model}...\")\n    \n    try:\n        # Check if Ollama is reachable (basic check, could be improved)\n        # Using langchain_ollama\n        llm = ChatOllama(model=model, temperature=0.7)\n        \n        prompt = ChatPromptTemplate.from_template(\"\"\"You are an expert Technical Content Creator.\n        \n        Source Transcription:\n        {transcription}\n        \n        Task:\n        Turn this raw transcription into a high-quality Technical Blog Post.\n        \n        Requirements:\n        1. Catchy Title.\n        2. Introduction summarizing the video content.\n        3. Key Takeaways (bullet points).\n        4. Detailed Technical Explanation (with code snippets if mentioned/inferred).\n        5. Conclusion.\n        \n        Format as Markdown.\n        \"\"\")\n        \n        chain = prompt | llm | StrOutputParser()\n        blog_post = chain.invoke({\"transcription\": transcription})\n        return blog_post\n        \n    except Exception as e:\n        return f\"Blog Generation Failed: {str(e)}\"\n\ndef save_to_docs(content: str, filename: str = \"video_blog.md\"):\n    \"\"\"\n    Saves the generated content to the /docs folder.\n    \"\"\"\n    docs_dir = os.path.join(os.getcwd(), \"docs\")\n    os.makedirs(docs_dir, exist_ok=True)\n    file_path = os.path.join(docs_dir, filename)\n    \n    with open(file_path, \"w\") as f:\n        f.write(content)\n        \n    print(f\"\ud83d\udcbe Saved blog post to: {file_path}\")\n    return file_path\n"}
{"text": "File: analytics.ts\n\n// Analytics tracking utility\nexport interface ChatMessage {\n    role: 'user' | 'model';\n    content: string;\n    timestamp: number;\n    tokenCount?: number;\n}\n\nexport interface AnalyticsData {\n    totalChats: number;\n    totalTokens: number;\n    totalMessages: number;\n    averageResponseTime: number;\n    chatHistory: ChatMessage[];\n}\n\nexport class Analytics {\n    private static readonly STORAGE_KEY = 'nexus_analytics';\n\n    static getData(): AnalyticsData {\n        if (typeof window === 'undefined') {\n            return this.getDefaultData();\n        }\n\n        try {\n            const data = localStorage.getItem(this.STORAGE_KEY);\n            return data ? JSON.parse(data) : this.getDefaultData();\n        } catch {\n            return this.getDefaultData();\n        }\n    }\n\n    static saveData(data: AnalyticsData): void {\n        if (typeof window === 'undefined') return;\n        localStorage.setItem(this.STORAGE_KEY, JSON.stringify(data));\n    }\n\n    static trackMessage(message: ChatMessage): void {\n        const data = this.getData();\n        data.chatHistory.push(message);\n        data.totalMessages++;\n        if (message.tokenCount) {\n            data.totalTokens += message.tokenCount;\n        }\n        this.saveData(data);\n    }\n\n    static trackNewChat(): void {\n        const data = this.getData();\n        data.totalChats++;\n        this.saveData(data);\n    }\n\n    private static getDefaultData(): AnalyticsData {\n        return {\n            totalChats: 0,\n            totalTokens: 0,\n            totalMessages: 0,\n            averageResponseTime: 1.2,\n            chatHistory: []\n        };\n    }\n}\n"}
{"text": "File: utils.ts\n\nimport { type ClassValue, clsx } from \"clsx\";\nimport { twMerge } from \"tailwind-merge\";\n\nexport function cn(...inputs: ClassValue[]) {\n  return twMerge(clsx(inputs));\n}\n"}
{"text": "File: generate_dataset.py\n\nimport os\nimport json\nimport random\n\ndef generate_dataset(root_dir, output_dir, split_ratio=0.9):\n    data = []\n    extensions = ('.ts', '.js', '.py')\n    exclude_dirs = {'.git', 'node_modules', '.next', 'chroma_db', 'playwright-report', 'test-results', '__pycache__', 'models'}\n\n    for root, dirs, files in os.walk(root_dir):\n        # Prune excluded directories\n        dirs[:] = [d for d in dirs if d not in exclude_dirs]\n        \n        for file in files:\n            if file.endswith(extensions):\n                file_path = os.path.join(root, file)\n                try:\n                    with open(file_path, 'r', encoding='utf-8') as f:\n                        content = f.read()\n                        if content.strip():\n                            # Format for mlx-lm: a JSON object with a \"text\" key\n                            data.append({\"text\": f\"File: {file}\\n\\n{content}\"})\n                except Exception as e:\n                    print(f\"Error reading {file_path}: {e}\")\n\n    random.shuffle(data)\n    split_idx = int(len(data) * split_ratio)\n    train_data = data[:split_idx]\n    valid_data = data[split_idx:]\n\n    os.makedirs(output_dir, exist_ok=True)\n\n    with open(os.path.join(output_dir, 'train.jsonl'), 'w', encoding='utf-8') as f:\n        for entry in train_data:\n            f.write(json.dumps(entry) + '\\n')\n\n    with open(os.path.join(output_dir, 'valid.jsonl'), 'w', encoding='utf-8') as f:\n        for entry in valid_data:\n            f.write(json.dumps(entry) + '\\n')\n\n    print(f\"Generated {len(train_data)} training samples and {len(valid_data)} validation samples.\")\n    print(f\"Dataset saved to {output_dir}\")\n\nif __name__ == \"__main__\":\n    project_root = \"/Users/rohitchandra/Documents/AI\"\n    output_path = os.path.join(project_root, \"data\")\n    generate_dataset(project_root, output_path)\n"}
{"text": "File: verify_creator.py\n\nimport sys\nimport os\nimport time\nimport requests\n\n# Add current directory to path\nsys.path.append(os.getcwd())\n\ndef test_whisper_import():\n    try:\n        print(\"Testing Whisper import...\")\n        from backend.creator.transcribe import transcribe_video\n        print(\"\u2705 Whisper import successful.\")\n        return True\n    except Exception as e:\n        print(f\"\u274c Whisper import failed: {e}\")\n        return False\n\ndef test_ollama_connection():\n    print(\"Testing Ollama connection...\")\n    try:\n        response = requests.get(\"http://localhost:11434/api/tags\")\n        if response.status_code == 200:\n            print(\"\u2705 Ollama is running and reachable.\")\n            print(f\"Models available: {response.json().get('models', [])}\")\n            # Check for llama3\n            models = [m['name'] for m in response.json().get('models', [])]\n            if any(\"llama3\" in m for m in models):\n                print(\"\u2705 Llama3 model found.\")\n            else:\n                print(\"\u26a0\ufe0f Llama3 model NOT found. Please run 'ollama pull llama3'.\")\n            return True\n        else:\n            print(f\"\u274c Ollama returned status {response.status_code}\")\n            return False\n    except Exception as e:\n        print(f\"\u274c Ollama connection failed: {e}\")\n        return False\n\nif __name__ == \"__main__\":\n    whisper_ok = test_whisper_import()\n    # Wait a bit for Ollama to fully start if it just launched\n    time.sleep(2)\n    ollama_ok = test_ollama_connection()\n    \n    if whisper_ok and ollama_ok:\n        print(\"\\n\ud83c\udf89 Nexus Creator verification successful (Dependencies ready).\")\n    else:\n        print(\"\\n\u26a0\ufe0f Nexus Creator verification incomplete.\")\n"}
{"text": "File: test_researcher.py\n\nimport os\nimport sys\n\n# Add current directory to path\nsys.path.append(os.getcwd())\n\ntry:\n    print(\"Testing import...\")\n    from backend.researcher.agent import run_researcher\n    print(\"Import successful.\")\n    \n    print(\"Testing execution (Forcing Fallback)...\")\n    if \"TAVILY_API_KEY\" in os.environ:\n        del os.environ[\"TAVILY_API_KEY\"]\n        \n    try:\n        result = run_researcher(\"Test query\")\n        print(\"Execution successful:\", result)\n    except Exception as e:\n        print(f\"Execution failed: {e}\")\n        import traceback\n        traceback.print_exc()\n\nexcept Exception as e:\n    print(f\"Import failed: {e}\")\n    import traceback\n    traceback.print_exc()\n"}
{"text": "File: social_media.py\n\nimport os\nimport concurrent.futures\n\n# Configuration\nTEACHER_MODEL = \"llama3:70b\"\nSD_MODEL_ID = \"stabilityai/stable-diffusion-xl-base-1.0\"\nWHISPER_MODEL = \"large-v3\"\nOUTPUT_DIR = \"backend/static/social\"\n\nclass SocialMediaAgent:\n    def __init__(self):\n        import psutil\n        os.makedirs(OUTPUT_DIR, exist_ok=True)\n        mem = psutil.virtual_memory()\n        print(f\"\ud83d\ude80 Nexus Social: Initialized. Memory: {mem.used/1e9:.1f}GB / {mem.total/1e9:.1f}GB\", flush=True)\n\n    def generate_text(self, feature_desc):\n        try:\n            from langchain_ollama import ChatOllama\n            from langchain_core.prompts import ChatPromptTemplate\n            from langchain_core.output_parsers import StrOutputParser\n            print(\"\u270d\ufe0f Generating Copywriting with Llama-3-70b...\", flush=True)\n            llm = ChatOllama(model=TEACHER_MODEL, temperature=0.8)\n            prompt = ChatPromptTemplate.from_template(\"\"\"\n            You are a World-Class Tech Evangelist. \n            Write a 500-word highly engaging LinkedIn post and a 5-tweet Twitter thread about this new feature:\n            {feature}\n            \n            Focus on:\n            1. Innovation and Impact.\n            2. Technical depth for developers.\n            3. Visionary future outlook.\n            \"\"\")\n            chain = prompt | llm | StrOutputParser()\n            res = chain.invoke({\"feature\": feature_desc})\n            \n            # Save to disk\n            with open(os.path.join(OUTPUT_DIR, \"copy.txt\"), \"w\") as f:\n                f.write(res)\n                \n            print(f\"\u2705 Llama-3-70b Copy Generated and Saved ({len(res)} chars).\", flush=True)\n            return res\n        except Exception as e:\n            print(f\"\u274c Text Generation Failed: {e}\", flush=True)\n            return None\n\n    def generate_images(self, feature_desc):\n        try:\n            import torch\n            from diffusers import StableDiffusionXLPipeline\n            device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n            print(f\"\ud83c\udfa8 Generating SDXL Cyberpunk Coding Images on {device}...\", flush=True)\n            pipe = StableDiffusionXLPipeline.from_pretrained(\n                SD_MODEL_ID, \n                torch_dtype=torch.float16, \n                variant=\"fp16\", \n                use_safetensors=True\n            ).to(device)\n            \n            prompt = f\"Nexus-AI brand dashboard, Cyberpunk tech aesthetic, high-end software architecture, deep blue and violet neon, cinematic composition, 8k, related to: {feature_desc}\"\n            paths = []\n            for i in range(3):\n                print(f\"\ud83d\uddbc\ufe0f Rendering Image {i+1}/3...\", flush=True)\n                image = pipe(prompt=prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\n                path = os.path.join(OUTPUT_DIR, f\"image_{i}.png\")\n                image.save(path)\n                paths.append(path)\n                \n            # Clear VRAM\n            del pipe\n            torch.mps.empty_cache()\n            return paths\n        except Exception as e:\n            print(f\"\u274c Image Generation Failed: {e}\", flush=True)\n            return []\n\n    def generate_voiceover(self, content):\n        try:\n            import torch\n            import whisper\n            # Whisper on MPS sometimes hits Sparse tensor issues, fallback to CPU if needed\n            device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n            print(f\"\ud83c\udf99\ufe0f Generating Voiceover Script with Whisper-Large-v3 on {device}...\", flush=True)\n            \n            try:\n                model = whisper.load_model(WHISPER_MODEL, device=device)\n            except Exception as e:\n                print(f\"\u26a0\ufe0f MPS failed for Whisper, falling back to CPU: {e}\", flush=True)\n                model = whisper.load_model(WHISPER_MODEL, device=\"cpu\")\n\n            print(\"\u2705 Whisper Loaded. Finalizing script...\", flush=True)\n            script = f\"NEXUS-AI BROADCAST SCRIPT:\\n\\n{content[:1000]}\"\n            \n            # Save to disk\n            with open(os.path.join(OUTPUT_DIR, \"script.txt\"), \"w\") as f:\n                f.write(script)\n                \n            print(f\"\u2705 Voiceover Script Generated and Saved ({len(script)} chars).\", flush=True)\n            \n            # Clear VRAM\n            del model\n            torch.mps.empty_cache()\n            return script\n        except Exception as e:\n            print(f\"\u274c Voiceover Generation Failed: {e}\", flush=True)\n            return None\n\n    def run_autonomous_flow(self, feature_desc):\n        results = {}\n        print(f\"\ud83d\udd25 Starting CONCURRENT Multi-Modal Flow for: {feature_desc}\")\n        \n        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n            future_text = executor.submit(self.generate_text, feature_desc)\n            future_images = executor.submit(self.generate_images, feature_desc)\n            future_voice = executor.submit(self.generate_voiceover, feature_desc)\n            \n            concurrent.futures.wait([future_text, future_images, future_voice])\n            \n            results['text'] = future_text.result()\n            results['images'] = future_images.result()\n            results['voiceover'] = future_voice.result()\n            \n        print(\"\u2728 Social Media Campaign Aggregated.\")\n        return results\n\nif __name__ == \"__main__\":\n    print(\"\ud83d\ude80 Nexus Social Media Agent: STARTING...\", flush=True)\n    agent = SocialMediaAgent()\n    print(\"\u2705 Agent Class Initialized.\", flush=True)\n    try:\n        results = agent.run_autonomous_flow(\"A revolutionary Graph-RAG system for codebase impact analysis using Neo4j and Llama-3.\")\n        print(\"\u2705 Flow Complete. Campaign Assets in backend/static/social/\", flush=True)\n    except Exception as e:\n        print(f\"\u274c FLOW FAILED: {e}\", flush=True)\n    finally:\n        print(\"\ud83c\udfc1 Social Agent Script Finished.\", flush=True)\n"}
{"text": "File: train_student.py\n\nimport sys\nfrom mlx_lm import load, lora\nfrom datasets import load_dataset\n\n# Configuration\n# MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\" # Can be fetched from HF\nMODEL_PATH = \"microsoft/Phi-3-mini-4k-instruct\"\nDATA_PATH = \"backend/distillation/data/golden_dataset.jsonl\"\nADAPTER_PATH = \"backend/distillation/adapters\"\n\ndef train_student():\n    print(f\"\ud83c\udf93 Nexus Student: Loading {MODEL_PATH} for fine-tuning...\")\n    \n    # 1. Load Model & Tokenizer\n    model, tokenizer = load(MODEL_PATH)\n    \n    # 2. Freeze Base Model (LoRA setup)\n    # MLX handles this implicitly in lora.train, but we configure it via args.\n    \n    print(\"\ud83d\udcc2 Loading Golden Dataset...\")\n    # Load JSONL dataset\n    dataset = load_dataset(\"json\", data_files=DATA_PATH, split=\"train\")\n    \n    # Minimal Training Configuration (Hardcoded for simplicity)\n    # real mlx_lm usage often involves CLI, but we can call the library functions directly if exposed,\n    # or wrap the CLI command. For stability, we'll construct a command to run mlx_lm.lora.\n    \n    print(\"\u26a0\ufe0f MLX Training is best run via CLI for full control.\")\n    print(\"Executing: python -m mlx_lm.lora ...\")\n    \n    cmd = [\n        sys.executable, \"-m\", \"mlx_lm.lora\",\n        \"--model\", MODEL_PATH,\n        \"--train\",\n        \"--data\", \"backend/distillation/data/\", # Directory containing jsonl\n        \"--batch-size\", \"4\",\n        \"--lora-layers\", \"16\",\n        \"--iters\", \"1000\",\n        \"--adapter-path\", ADAPTER_PATH\n    ]\n    \n    import subprocess\n    subprocess.run(cmd)\n\nif __name__ == \"__main__\":\n    train_student()\n"}
{"text": "File: useLocalStorage.ts\n\nimport { useState, useEffect } from 'react';\n\nexport function useLocalStorage<T>(key: string, initialValue: T): [T, (value: T | ((val: T) => T)) => void] {\n    // State to store our value\n    const [storedValue, setStoredValue] = useState<T>(() => {\n        if (typeof window === 'undefined') {\n            return initialValue;\n        }\n        try {\n            const item = window.localStorage.getItem(key);\n            return item ? JSON.parse(item) : initialValue;\n        } catch (error) {\n            console.log(error);\n            return initialValue;\n        }\n    });\n\n    // Return a wrapped version of useState's setter function that persists to localStorage\n    const setValue = (value: T | ((val: T) => T)) => {\n        try {\n            const valueToStore = value instanceof Function ? value(storedValue) : value;\n            setStoredValue(valueToStore);\n            if (typeof window !== 'undefined') {\n                window.localStorage.setItem(key, JSON.stringify(valueToStore));\n            }\n        } catch (error) {\n            console.log(error);\n        }\n    };\n\n    return [storedValue, setValue];\n}\n"}
{"text": "File: dry_run_audit.py\n\nimport os\nimport concurrent.futures\nimport json\nimport requests\nimport time\n\n# Configuration for DRY RUN\nMODEL = \"phi3:mini\"\nOUTPUT_FILE = \"dry_run_audit_report.md\"\nTARGET_DIRS = {\n    \"Security\": [\"backend/server.py\", \"app/api/\"],\n    \"Performance\": [\"app/\", \"components/\"],\n    \"Architecture\": [\"app/\", \"lib/\", \"backend/server.py\"],\n    \"Testing\": [\".\"]\n}\n\nclass CodeAuditOrchestrator:\n    def __init__(self):\n        print(f\"\ud83d\udd75\ufe0f  Dry Run Orchestrator: Initialized (Model: {MODEL})\")\n\n    def run_agent(self, agent_name, dirs):\n        print(f\"\ud83d\ude80 Launching Agent: {agent_name}...\")\n        \n        code_context = \"\"\n        for d in dirs:\n            if os.path.isfile(d):\n                with open(d, 'r') as f:\n                    code_context += f\"\\nFILE: {d}\\n{f.read()[:500]}\\n\"\n            elif os.path.isdir(d):\n                for root, _, files in os.walk(d):\n                    for file in files[:2]: \n                        if file.endswith(('.ts', '.tsx', '.py', '.js')):\n                            path = os.path.join(root, file)\n                            with open(path, 'r') as f:\n                                code_context += f\"\\nFILE: {path}\\n{f.read()[:500]}\\n\"\n\n        prompts = {\n            \"Security\": \"Check for obvious security issues.\",\n            \"Performance\": \"Check for obvious performance issues.\",\n            \"Architecture\": \"Suggest server actions migration.\",\n            \"Testing\": \"Generate a simple playwright test.\"\n        }\n\n        prompt = f\"{prompts[agent_name]}\\n\\nCODE CONTEXT:\\n{code_context}\"\n        \n        try:\n            response = requests.post(\"http://localhost:11434/api/generate\", \n                                   json={\"model\": MODEL, \"prompt\": prompt, \"stream\": False},\n                                   timeout=30)\n            res_json = response.json()\n            return f\"## Agent: {agent_name} Result\\n\\n{res_json['response']}\\n\\n\"\n        except Exception as e:\n            return f\"## Agent: {agent_name} Result\\n\\nFAILED: {e}\\n\\n\"\n\n    def execute_audit(self):\n        print(f\"\ud83d\udd25 Starting DRY RUN...\")\n        results = []\n        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n            futures = {executor.submit(self.run_agent, name, dirs): name for name, dirs in TARGET_DIRS.items()}\n            for future in concurrent.futures.as_completed(futures):\n                results.append(future.result())\n\n        with open(OUTPUT_FILE, \"w\") as f:\n            f.write(\"# Dry Run Audit Report\\n\\n\" + \"\".join(results))\n        print(f\"\u2728 Dry Run Complete. Report: {OUTPUT_FILE}\")\n\nif __name__ == \"__main__\":\n    orchestrator = CodeAuditOrchestrator()\n    orchestrator.execute_audit()\n"}
{"text": "File: useNeuralState.ts\n\nimport { useState, useEffect } from 'react';\nimport { neuralCore, NeuralState } from '@/lib/neural-core';\n\nexport function useNeuralState() {\n    const [state, setState] = useState<NeuralState>({\n        isLoading: false,\n        progress: 0,\n        text: \"Idle\",\n        isReady: false,\n        error: null,\n        excitement: 0,\n        stress: 0\n    });\n\n    useEffect(() => {\n        const unsubscribe = neuralCore.subscribe(setState);\n        return () => unsubscribe();\n    }, []);\n\n    return state;\n}\n"}
{"text": "File: verify_researcher_mock.py\n\nimport os\nimport sys\nfrom unittest.mock import MagicMock\n\n# Add current directory to path\nsys.path.append(os.getcwd())\n\n# Mock Tavily Search Tool to bypass API Key\ndef mock_search(query):\n    return [{\"url\": \"https://example.com\", \"content\": \"Next.js 15 Server Actions simplify backend logic.\"}]\n\n# Mock the entire TavilySearchResults class inside the module?\n# Better to patch it in the function call, but for simplicity here we just check import\n# and basic logic structure.\n\ndef verify_researcher_structure():\n    try:\n        print(\"Testing Researcher Agent Import...\")\n        from backend.researcher.agent import run_researcher, workflow\n        print(\"\u2705 Import successful.\")\n        \n        # Check workflow graph\n        print(f\"\u2705 Workflow compiled: {workflow}\")\n        \n        # We can't easily run it without API Key unless we mock everything deeply.\n        # But import success confirms the code structure is valid (LangGraph, etc.)\n        return True\n    except Exception as e:\n        print(f\"\u274c Researcher Agent Import/Structure Failed: {e}\")\n        return False\n\nif __name__ == \"__main__\":\n    if verify_researcher_structure():\n        print(\"\\n\ud83c\udf89 Nexus Researcher Logic Verified (Pending API Key).\")\n    else:\n        print(\"\\n\u26a0\ufe0f Nexus Researcher Verification Failed.\")\n"}
{"text": "File: agent.py\n\nimport os\nfrom typing import TypedDict, List, Annotated\nfrom langgraph.graph import StateGraph, END\nfrom langchain_core.messages import HumanMessage, SystemMessage, BaseMessage\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom langchain_community.tools.tavily_search import TavilySearchResults\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\n# --- Configuration ---\nGOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\") or \"AIzaSyB0XcmiVCVDen-Qk0pwgadEiN2aAUGLkKU\"\nTAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n\nif not TAVILY_API_KEY:\n    print(\"\u26a0\ufe0f Tavily API Key missing. Researcher will fail.\")\n\n# --- LLM Setup ---\nif not GOOGLE_API_KEY:\n    print(\"\u26a0\ufe0f GOOGLE_API_KEY missing. Researcher analysis will fail.\")\n\nllm_pro = ChatGoogleGenerativeAI(\n    model=\"gemini-2.5-pro\",\n    google_api_key=GOOGLE_API_KEY,\n    convert_system_message_to_human=True,\n    temperature=0.2,\n    max_retries=0 # Fail fast\n)\n\nllm_flash = ChatGoogleGenerativeAI(\n    model=\"gemini-2.5-flash\",\n    google_api_key=GOOGLE_API_KEY,\n    convert_system_message_to_human=True,\n    temperature=0.2\n)\n\n# --- State Design ---\nclass ResearcherState(TypedDict):\n    query: str\n    search_results: str\n    analysis: str\n    migration_plan: str\n    messages: List[BaseMessage]\n\n# --- Nodes ---\n\ndef search_node(state: ResearcherState):\n    \"\"\"\n    Searches the web using Tavily (Primary) or DuckDuckGo (Fallback).\n    \"\"\"\n    query = state[\"query\"]\n    print(f\"\ud83c\udf0d Researcher: Searching for '{query}'...\")\n    \n    try:\n        if TAVILY_API_KEY:\n            # Primary: Tavily\n            search_tool = TavilySearchResults(max_results=5)\n            results = search_tool.invoke(query)\n            context = \"\\\\n\".join([f\"- [{r['url']}]: {r['content']}\" for r in results])\n            return {\"search_results\": context}\n        else:\n            # Fallback: DuckDuckGo\n            from langchain_community.tools import DuckDuckGoSearchRun\n            print(\"\u26a0\ufe0f Tavily Key missing. Using DuckDuckGo Fallback.\")\n            search_tool = DuckDuckGoSearchRun()\n            # DDG returns a string directly usually, or we wrap it\n            results = search_tool.invoke(query)\n            return {\"search_results\": f\"DuckDuckGo Results:\\\\n{results}\"}\n            \n    except Exception as e:\n        return {\"search_results\": f\"Search Failed: {str(e)}\"}\n\ndef analyze_node(state: ResearcherState):\n    \"\"\"\n    Compares search findings with generic codebase knowledge (package.json).\n    \"\"\"\n    print(\"\ud83e\udde0 Researcher: Analyzing findings...\")\n    query = state[\"query\"]\n    results = state[\"search_results\"]\n    \n    # Read package.json for context\n    try:\n        with open(\"package.json\", \"r\") as f:\n            package_json = f.read()\n    except:\n        package_json = \"No package.json found.\"\n    \n    prompt = ChatPromptTemplate.from_template(\"\"\"You are a Senior Technical Researcher.\n    \n    User Query: {query}\n    \n    Current Project Definition (package.json):\n    {package_json}\n    \n    Latest Web Findings:\n    {results}\n    \n    Task:\n    Analyze these findings. Identify the key technical patterns, best practices, and major changes (breaking changes) introduced in these new technologies.\n    Contrast this with the current project dependencies found in package.json (e.g. current Next.js version vs latest).\n    \n    Output a concise analysis.\n    \"\"\")\n    \n    chain_pro = prompt | llm_pro | StrOutputParser()\n    chain_flash = prompt | llm_flash | StrOutputParser()\n    \n    try:\n        analysis = chain_pro.invoke({\"query\": query, \"results\": results, \"package_json\": package_json})\n    except Exception as e:\n        print(f\"\u26a0\ufe0f Pro Model Failed (Analyze): {e}. Switching to Flash.\")\n        analysis = chain_flash.invoke({\"query\": query, \"results\": results, \"package_json\": package_json})\n        \n    return {\"analysis\": analysis}\n\ndef plan_node(state: ResearcherState):\n    \"\"\"\n    Proposes a migration plan based on the analysis.\n    \"\"\"\n    print(\"\ud83d\udccb Researcher: Drafting migration plan...\")\n    analysis = state[\"analysis\"]\n    \n    prompt = ChatPromptTemplate.from_template(\"\"\"You are a Software Architect.\n    \n    Research Analysis:\n    {analysis}\n    \n    Task:\n    Propose a concrete 'Migration Plan' to update a project to these new standards.\n    Structure it as:\n    1. Executive Summary\n    2. Key Changes Required\n    3. Step-by-Step Migration Strategy\n    4. Pitfalls to Avoid\n    \n    Format as Markdown.\n    \"\"\")\n    \n    chain_pro = prompt | llm_pro | StrOutputParser()\n    chain_flash = prompt | llm_flash | StrOutputParser()\n    \n    try:\n        plan = chain_pro.invoke({\"analysis\": analysis})\n    except Exception as e:\n        print(f\"\u26a0\ufe0f Pro Model Failed (Plan): {e}. Switching to Flash.\")\n        plan = chain_flash.invoke({\"analysis\": analysis})\n        \n    return {\"migration_plan\": plan}\n\n# --- Graph Definition ---\nworkflow = StateGraph(ResearcherState)\n\nworkflow.add_node(\"search\", search_node)\nworkflow.add_node(\"analyze\", analyze_node)\nworkflow.add_node(\"plan\", plan_node)\n\nworkflow.set_entry_point(\"search\")\nworkflow.add_edge(\"search\", \"analyze\")\nworkflow.add_edge(\"analyze\", \"plan\")\nworkflow.add_edge(\"plan\", END)\n\napp = workflow.compile()\n\ndef run_researcher(query: str):\n    \"\"\"\n    Entry point for the API.\n    \"\"\"\n    initial_state = {\"query\": query, \"messages\": [HumanMessage(content=query)]}\n    result = app.invoke(initial_state)\n    return result[\"migration_plan\"]\n"}
{"text": "File: next.config.ts\n\nimport type { NextConfig } from \"next\";\n\nconst nextConfig: NextConfig = {\n  /* config options here */\n};\n\nexport default nextConfig;\n"}
{"text": "File: ingest.py\n\nimport os\nimport shutil\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter, Language\nfrom langchain_community.vectorstores import Chroma\nfrom backend.rag import rag_service, CHROMA_PATH\n\n# Exclusions\nEXCLUDE_DIRS = {\n    \"node_modules\", \".git\", \".next\", \"venv\", \"__pycache__\", \n    \".gemini\", \"dist\", \"build\", \"coverage\", \".vscode\", \".idea\"\n}\n\n# Supported Extensions\nEXTENSIONS = {\".tsx\", \".ts\", \".py\", \".js\", \".json\", \".css\", \".md\"}\n\ndef crawl_codebase(root_dir):\n    documents = []\n    print(f\"\ud83d\udd0d Scanning codebase at {root_dir}...\")\n    \n    for dirpath, dirnames, filenames in os.walk(root_dir):\n        # Filter directories in-place\n        dirnames[:] = [d for d in dirnames if d not in EXCLUDE_DIRS]\n        \n        for file in filenames:\n            ext = os.path.splitext(file)[1]\n            if ext in EXTENSIONS:\n                file_path = os.path.join(dirpath, file)\n                try:\n                    loader = TextLoader(file_path, encoding='utf-8')\n                    documents.extend(loader.load())\n                except Exception as e:\n                    # Skip files that fail to load (binary, huge, etc)\n                    pass\n                    \n    return documents\n\ndef ingest_codebase():\n    root_path = os.path.abspath(\".\")\n    \n    # 1. Load Code\n    docs = crawl_codebase(root_path)\n    if not docs:\n        return {\"status\": \"error\", \"message\": \"No code files found.\"}\n    \n    print(f\"\u2705 Loaded {len(docs)} code files.\")\n\n    # 2. Split Code\n    # We use a generic splitter, but for optimal results we could use from_language\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=2000, # Larger chunks for context\n        chunk_overlap=200,\n        add_start_index=True,\n    )\n    chunks = text_splitter.split_documents(docs)\n    print(f\"\u2702\ufe0f Split into {len(chunks)} code chunks.\")\n\n    # 3. Add to ChromaDB (Reuse Client from RAG Service if available to avoid locks/errors)\n    COLLECTION_NAME = \"nexus_architect\"\n    \n    # Try to get client from initialized RAG service\n    client = None\n    if rag_service.vector_store:\n        try:\n            client = rag_service.vector_store._client\n            print(\"\ud83d\udd17 Reusing existing Chroma Client from RAG Service.\")\n        except:\n            pass\n            \n    if client:\n        # Use shared client\n        try:\n            # We can't easily delete collection via client if it's generic, \n            # but we can try client.delete_collection(COLLECTION_NAME)\n            try:\n                client.delete_collection(COLLECTION_NAME)\n            except:\n                pass\n                \n            vector_store = Chroma.from_documents(\n                documents=chunks, \n                embedding=rag_service.embeddings,\n                client=client,\n                collection_name=COLLECTION_NAME\n            )\n            return {\"status\": \"success\", \"chunks\": len(chunks), \"collection\": COLLECTION_NAME, \"method\": \"shared_client\"}\n        except Exception as e:\n            return {\"status\": \"error\", \"message\": f\"Shared Client Ingest Failed: {str(e)}\"}\n\n    # Fallback to independent init (Only if RAG service is dead)\n    chroma_host = os.getenv(\"CHROMA_SERVER_HOST\")\n    chroma_port = os.getenv(\"CHROMA_SERVER_PORT\", \"8000\")\n    \n    if chroma_host:\n        import chromadb\n        print(f\"\ud83d\udcbe Ingesting into ChromaDB Server ({COLLECTION_NAME})...\")\n        try:\n            client = chromadb.HttpClient(host=chroma_host, port=int(chroma_port))\n            try:\n                client.delete_collection(COLLECTION_NAME)\n            except:\n                pass\n                \n            vector_store = Chroma.from_documents(\n                documents=chunks, \n                embedding=rag_service.embeddings,\n                client=client,\n                collection_name=COLLECTION_NAME\n            )\n        except Exception as e:\n            return {\"status\": \"error\", \"message\": str(e)}\n            \n    else:\n        # Local Mode\n        print(f\"\ud83d\udcbe Ingesting into Local Storage ({COLLECTION_NAME})...\")\n        try:\n            import chromadb\n            client = chromadb.PersistentClient(path=CHROMA_PATH)\n            vector_store = Chroma(\n                client=client,\n                collection_name=COLLECTION_NAME,\n                embedding_function=rag_service.embeddings\n            )\n            vector_store.add_documents(documents=chunks)\n            print(f\"\u2705 Added {len(chunks)} chunks to {COLLECTION_NAME}\")\n        except Exception as e:\n             return {\"status\": \"error\", \"message\": f\"Local Ingest Failed: {str(e)}\"}\n\n    return {\"status\": \"success\", \"chunks\": len(chunks), \"collection\": COLLECTION_NAME}\n\nif __name__ == \"__main__\":\n    ingest_codebase()\n"}
{"text": "File: ingest.py\n\nimport os\nimport shutil\nfrom langchain_community.document_loaders import PyPDFLoader, DirectoryLoader, TextLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_community.vectorstores import Chroma\nfrom backend.rag import rag_service, CHROMA_PATH\n\n# User's Research folder + Current Repo\nDATA_PATHS = [\n    os.path.expanduser(\"~/Documents/Research\"), # Hypothetical path from prompt\n    os.path.abspath(\".\") # Current Repo\n]\n\ndef ingest_documents():\n    \"\"\"Scans Research folder and Repo.\"\"\"\n    \n    docs = []\n    \n    for path in DATA_PATHS:\n        if not os.path.exists(path):\n            print(f\"Path not found: {path}, skipping.\")\n            continue\n            \n        print(f\"Scanning {path}...\")\n        \n        # Loaders\n        # Exclude node_modules, .git, etc via glob if using DirectoryLoader is tricky, \n        # but RecursiveCharacterTextSplitter handles content. \n        # For simplicity, we'll try to load specific extensions.\n        \n        loaders = [\n            DirectoryLoader(path, glob=\"**/*.pdf\", loader_cls=PyPDFLoader, show_progress=True),\n            DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader, show_progress=True),\n            DirectoryLoader(path, glob=\"**/*.md\", loader_cls=TextLoader, show_progress=True),\n            DirectoryLoader(path, glob=\"**/*.tsx\", loader_cls=TextLoader, show_progress=True),\n            DirectoryLoader(path, glob=\"**/*.ts\", loader_cls=TextLoader, show_progress=True),\n            DirectoryLoader(path, glob=\"**/*.py\", loader_cls=TextLoader, show_progress=True)\n        ]\n        \n        for loader in loaders:\n            try:\n                loaded = loader.load()\n                docs.extend(loaded)\n                print(f\"Loaded {len(loaded)} docs from {loader.glob}\")\n            except Exception as e:\n                # directory loader sometimes fails on single files, ignore\n                continue\n\n    if not docs:\n        return {\"status\": \"error\", \"message\": \"No documents found.\"}\n\n    # 2. Split Text\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,\n        chunk_overlap=200,\n        add_start_index=True,\n    )\n    chunks = text_splitter.split_documents(docs)\n    print(f\"Split into {len(chunks)} chunks.\")\n\n    # 3. Add to ChromaDB\n    # 3. Add to ChromaDB\n    chroma_host = os.getenv(\"CHROMA_SERVER_HOST\")\n    chroma_port = os.getenv(\"CHROMA_SERVER_PORT\", \"8000\")\n\n    if chroma_host:\n        import chromadb\n        print(f\"Ingesting into ChromaDB Server at {chroma_host}:{chroma_port}...\")\n        try:\n            # Use HttpClient for remote server\n            client = chromadb.HttpClient(host=chroma_host, port=int(chroma_port))\n            try:\n                client.delete_collection(\"nexus_ai\")\n            except:\n                pass\n            \n            vector_store = Chroma.from_documents(\n                documents=chunks, \n                embedding=rag_service.embeddings,\n                client=client,\n                collection_name=\"nexus_ai\"\n            )\n        except Exception as e:\n            return {\"status\": \"error\", \"message\": f\"Server Connection Failed: {str(e)}\"}\n    \n    else:\n        # Local Mode using PersistentClient directly (Fixing 'http-only' error)\n        print(f\"Using Local Storage at {CHROMA_PATH}...\")\n        if os.path.exists(CHROMA_PATH):\n            shutil.rmtree(CHROMA_PATH)\n            \n        vector_store = Chroma.from_documents(\n            documents=chunks, \n            embedding=rag_service.embeddings,\n            persist_directory=CHROMA_PATH\n        )\n    \n    # Reload service\n    rag_service.initialize_vector_store()\n    \n    return {\"status\": \"success\", \"chunks\": len(chunks)}\n\nif __name__ == \"__main__\":\n    ingest_documents()\n"}
{"text": "File: server.py\n\nfrom fastapi import FastAPI, HTTPException, UploadFile, File\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import StreamingResponse\nfrom fastapi.staticfiles import StaticFiles\nfrom pydantic import BaseModel\nimport uvicorn\nimport asyncio\nimport os\n\n# Import backend modules\nfrom backend.rag import rag_service\nfrom backend.ingest import ingest_documents\nfrom backend.agents.workflow import run_feature_workflow\nfrom backend.vision import generate_code_from_image\nfrom backend.search_service import search_service\nfrom backend.architect.agent import architect_agent\nfrom backend.architect.ingest import ingest_codebase\n\n# --- Data Models (Must be defined before use) ---\nclass ChatRequest(BaseModel):\n    message: str\n\nclass AgentRequest(BaseModel):\n    feature: str\n\n# --- App Initialization ---\napp = FastAPI()\n\n# Enable CORS\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Static Files (Social Media Assets)\nos.makedirs(\"backend/static/social\", exist_ok=True)\napp.mount(\"/static/social\", StaticFiles(directory=\"backend/static/social\"), name=\"social\")\n\n# --- Routes ---\n\n@app.get(\"/\")\ndef read_root():    \n    return {\"status\": \"Nexus-AI Server Running\"}\n\n# 1. RAG Chat (Deep Research)\n@app.post(\"/api/chat\")\nasync def chat(request: ChatRequest):\n    async def generate():\n        try:\n            for chunk in rag_service.query(request.message):\n                yield chunk\n        except Exception as e:\n            yield f\"Error: {str(e)}\"\n\n    return StreamingResponse(generate(), media_type=\"text/plain\")\n\n# 2. General Assistant (Gemini Pro)\n@app.post(\"/api/general/chat\")\nasync def general_chat(request: ChatRequest):\n    async def generate():\n        try:\n            # Using Gemini Pro Service\n            for chunk in search_service.chat_stream(request.message):\n                yield chunk\n        except Exception as e:\n            yield f\"Error: {str(e)}\"\n\n    return StreamingResponse(generate(), media_type=\"text/plain\")\n\n# 3. RAG Ingest\n@app.post(\"/api/ingest\")\nasync def ingest():\n    try:\n        result = ingest_documents()\n        return result\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n# 4. Agent Workforce\n@app.post(\"/api/agents/generate\")\nasync def generate_agent(request: AgentRequest):\n    async def run_workflow():\n        try:\n            # We wrap the synchronous CrewAI/Lite call\n            result = run_feature_workflow(request.feature)\n            yield f\"Workflow Complete: {result}\"\n        except Exception as e:\n            yield f\"Error: {str(e)}\"\n\n    return StreamingResponse(run_workflow(), media_type=\"text/plain\")\n\n# 5. Vision Projector\n@app.post(\"/api/vision/generate\")\nasync def vision_generate(file: UploadFile = File(...)):\n    async def process_vision():\n        try:\n            contents = await file.read()\n            # Stream the response from the vision model\n            for chunk in generate_code_from_image(contents):\n                yield chunk\n        except Exception as e:\n            yield f\"Error: {str(e)}\"\n\n    return StreamingResponse(process_vision(), media_type=\"text/plain\")\n\n    return StreamingResponse(process_vision(), media_type=\"text/plain\")\n\n# 6. Nexus Architect\n@app.post(\"/api/architect/ingest\")\nasync def architect_ingest_route():\n    try:\n        result = ingest_codebase()\n        return result\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/api/architect/audit\")\nasync def architect_audit_route():\n    async def generate_report():\n        try:\n            for chunk in architect_agent.run_audit():\n                yield chunk\n        except Exception as e:\n            yield f\"Error: {str(e)}\"\n            \n    return StreamingResponse(generate_report(), media_type=\"text/plain\")\n\n# 7. Nexus Researcher\nclass ResearchRequest(BaseModel):\n    query: str\n\n@app.post(\"/api/researcher/run\")\nasync def run_research(request: ResearchRequest):\n    \"\"\"\n    Executes the Nexus Researcher agent.\n    \"\"\"\n    # Lazy import to avoid potential circular dependency issues during startup\n    from backend.researcher.agent import run_researcher\n    try:\n        report = run_researcher(request.query)\n        return {\"status\": \"success\", \"report\": report}\n    except Exception as e:\n        return {\"status\": \"error\", \"message\": str(e)}\n\n# 8. Nexus Creator (Video based Blog)\n@app.post(\"/api/creator/upload\")\nasync def creator_upload(file: UploadFile = File(...)):\n    \"\"\"\n    Processes a video file: Transcribe (Whisper) -> Blog (Llama-3).\n    \"\"\"\n    import shutil\n    from backend.creator.transcribe import transcribe_video\n    from backend.creator.blog_generator import generate_blog_post, save_to_docs\n\n    try:\n        # 1. Save File\n        upload_dir = \"uploads\"\n        os.makedirs(upload_dir, exist_ok=True)\n        file_location = f\"{upload_dir}/{file.filename}\"\n        \n        with open(file_location, \"wb\") as buffer:\n            shutil.copyfileobj(file.file, buffer)\n            \n        # 2. Transcribe\n        transcription = transcribe_video(file_location)\n        \n        # 3. Generate Blog\n        blog_content = generate_blog_post(transcription)\n        \n        # 4. Save to /docs\n        saved_path = save_to_docs(blog_content, filename=f\"blog_{file.filename}.md\")\n        \n        return {\n            \"status\": \"success\", \n            \"transcription\": transcription[:500] + \"...\", \n            \"blog_content\": blog_content,\n            \"saved_path\": saved_path\n        }\n    except Exception as e:\n        return {\"status\": \"error\", \"message\": f\"Creator Flow Failed: {str(e)}\"}\n\n# 9. Graph-RAG Impact Analysis\nclass GraphRequest(BaseModel):\n    question: str\n\n@app.post(\"/api/graph/impact\")\nasync def graph_impact_route(request: GraphRequest):\n    \"\"\"\n    Analyzes system impact using Neo4j Knowledge Graph.\n    \"\"\"\n    from backend.graph.query import GraphRAG\n    rag = GraphRAG()\n    try:\n        answer = rag.run_query(request.question)\n        return {\"status\": \"success\", \"answer\": answer}\n    except Exception as e:\n        return {\"status\": \"error\", \"message\": f\"Graph Query Failed: {str(e)}\"}\n    finally:\n        rag.close()\n\n# 10. Autonomous Social Media Agent\nclass SocialRequest(BaseModel):\n    feature_desc: str\n\n@app.post(\"/api/agents/social\")\nasync def social_agent_route(request: SocialRequest):\n    \"\"\"\n    Generates multi-modal social media content via SDXL, Whisper, and Llama-3.\n    \"\"\"\n    from backend.agents.social_media import SocialMediaAgent\n    agent = SocialMediaAgent()\n    try:\n        results = agent.run_autonomous_flow(request.feature_desc)\n        return {\"status\": \"success\", \"results\": results}\n    except Exception as e:\n        return {\"status\": \"error\", \"message\": f\"Social Agent Failed: {str(e)}\"}\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"}
{"text": "File: crew_lite.py\n\nfrom typing import List, Any\nfrom pydantic import BaseModel\nfrom langchain.chat_models.base import BaseChatModel\nfrom langchain.schema import HumanMessage, SystemMessage\n\nclass Agent(BaseModel):\n    role: str\n    goal: str\n    backstory: str\n    tools: List[Any] = []\n    llm: Any\n    verbose: bool = False\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    def execute(self, task_description: str, context: str = \"\") -> str:\n        prompt = f\"\"\"\nYou are a {self.role}.\nGoal: {self.goal}\nBackstory: {self.backstory}\n\nYour Task: {task_description}\n\nContext from previous steps:\n{context}\n\nYou have access to tools, but for this interaction, please output the final result directly.\nIf you need to use a tool, describe the action you would take, but primarily focus on generating the content requested (Code, Test, or Report).\n\"\"\"\n        messages = [\n            SystemMessage(content=self.backstory),\n            HumanMessage(content=prompt)\n        ]\n        \n        response = self.llm.invoke(messages)\n        return response.content\n\nclass Task(BaseModel):\n    description: str\n    agent: Agent\n    expected_output: str\n\nclass Crew(BaseModel):\n    agents: List[Agent]\n    tasks: List[Task]\n    verbose: int = 0\n\n    def kickoff(self):\n        context = \"\"\n        results = []\n        \n        for i, task in enumerate(self.tasks):\n            print(f\"Starting Task {i+1}: {task.description}\")\n            result = task.agent.execute(task.description, context)\n            results.append(result)\n            context += f\"\\n\\nOutput from {task.agent.role}:\\n{result}\"\n            print(f\"Task {i+1} Complete.\")\n            \n        return \"Workflow Completed Successfully.\\n\" + context\n\nclass Process:\n    sequential = \"sequential\"\n"}
{"text": "File: radix-icons.d.ts\n\ndeclare module '@radix-ui/react-icons' {\n    import * as React from 'react';\n    export const DragHandleDots2Icon: React.FC<React.SVGProps<SVGSVGElement>>;\n    // Add other icons as needed or use a catch-all\n    export const PlusIcon: React.FC<React.SVGProps<SVGSVGElement>>;\n    export const TrashIcon: React.FC<React.SVGProps<SVGSVGElement>>;\n    // ... generic fallback\n    const content: { [key: string]: React.FC<React.SVGProps<SVGSVGElement>> };\n    export default content;\n}\n"}
{"text": "File: check_chroma_content.py\n\nimport chromadb\nimport os\nimport sys\n\n# Add path to find backend\nsys.path.append(os.getcwd())\nfrom backend.rag import CHROMA_PATH\n\nprint(f\"Checking ChromaDB at: {CHROMA_PATH}\")\n\nif not os.path.exists(CHROMA_PATH):\n    print(\"\u274c Path does not exist!\")\n    sys.exit(1)\n\ntry:\n    client = chromadb.PersistentClient(path=CHROMA_PATH)\n    collections = client.list_collections()\n    print(f\"Found {len(collections)} collections:\")\n    for col in collections:\n        print(f\" - {col.name} (Count: {col.count()})\")\n        \n    # Check specifically nexus_architect\n    try:\n        col = client.get_collection(\"nexus_architect\")\n        print(f\"\u2705 'nexus_architect' exists with {col.count()} docs.\")\n        if col.count() == 0:\n             print(\"\u26a0\ufe0f Collection is empty!\")\n    except Exception as e:\n        print(f\"\u274c 'nexus_architect' not found: {e}\")\n\nexcept Exception as e:\n    print(f\"Error accessing DB: {e}\")\n"}
