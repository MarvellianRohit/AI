{"text": "File: indexer.py\n\nimport os\nimport glob\nimport json\nimport concurrent.futures\nfrom pathlib import Path\nfrom neo4j import GraphDatabase\nfrom langchain_ollama import ChatOllama\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import JsonOutputParser\n\n# Configuration\nPROJECT_ROOT = \"/Users/rohitchandra/Documents\"\nPROJECT_DIRS = [\"AI\", \"E-Commerce App\", \"Food Delivery App\", \"college\"]\nNEO4J_URI = \"bolt://localhost:7687\"\nNEO4J_USER = \"neo4j\"\nNEO4J_PASSWORD = \"password123\"\nTEACHER_MODEL = \"phi3:mini\"\nMAX_WORKERS = 1\n\nclass CodeGraphIndexer:\n    def __init__(self):\n        print(f\"\ud83d\udd17 Connecting to Neo4j at {NEO4J_URI}...\")\n        self.driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n        print(\"\ud83e\udd16 Initializing LLM (phi3:mini)...\")\n        self.llm = ChatOllama(model=TEACHER_MODEL, temperature=0)\n        self.parser = JsonOutputParser()\n        \n        self.extraction_prompt = ChatPromptTemplate.from_template(\"\"\"\n        Extract Entities and Relationships from code.\n        \n        File: {filename}\n        Code: {code}\n        \n        Output valid JSON with \"entities\" and \"relationships\".\n        \"\"\")\n\n    def close(self):\n        self.driver.close()\n\n    def run_cypher(self, query, params=None):\n        with self.driver.session() as session:\n            session.run(query, params or {})\n\n    def extract_from_file(self, file_path):\n        import time\n        max_retries = 1\n        for attempt in range(max_retries + 1):\n            try:\n                filename = os.path.relpath(file_path, PROJECT_ROOT)\n                print(f\"\ud83d\udcc4 Processing {filename} (Attempt {attempt+1})...\")\n                \n                with open(file_path, 'r', encoding='utf-8') as f:\n                    code = f.read()\n                \n                code_snippet = code[:1500] \n                \n                print(f\"\ud83e\udde0 Prompting LLM for {filename}...\")\n                chain = self.extraction_prompt | self.llm\n                raw_response = chain.invoke({\"filename\": filename, \"code\": code_snippet})\n                print(f\"\ud83d\udce5 Received Response for {filename}: {str(raw_response.content)[:100]}...\")\n                \n                data = self.parser.parse(raw_response.content)\n                self.ingest_graph(filename, data)\n                return f\"\u2705 Indexed {filename}\"\n            except Exception as e:\n                print(f\"\u26a0\ufe0f Error in {file_path}: {e}\")\n                time.sleep(1)\n        return f\"\u274c Failed {file_path}.\"\n\n    def ingest_graph(self, filename, data):\n        # 1. Ensure File Node\n        self.run_cypher(\"MERGE (f:File {name: $name})\", {\"name\": filename})\n        \n        # 2. Ingest Entities\n        for entity in data.get(\"entities\", []):\n            label = entity.get(\"type\", \"Entity\")\n            name = entity.get(\"name\")\n            if name:\n                self.run_cypher(\n                    f\"MERGE (e:{label} {{name: $name}}) MERGE (f:File {{name: $file}}) MERGE (f)-[:DEFINES]->(e)\",\n                    {\"name\": name, \"file\": filename}\n                )\n        \n        # 3. Ingest Relationships\n        for rel in data.get(\"relationships\", []):\n            source = rel.get(\"source\")\n            target = rel.get(\"target\")\n            rel_type = rel.get(\"type\", \"DEPENDS_ON\")\n            if source and target:\n                self.run_cypher(\n                    f\"MERGE (s {{name: $source}}) MERGE (t {{name: $target}}) MERGE (s)-[:{rel_type}]->(t)\",\n                    {\"source\": source, \"target\": target}\n                )\n\n    def scan_and_index(self):\n        # Target important file types\n        extensions = ['*.ts', '*.tsx', '*.py', '*.js']\n        files = []\n        for pdir in PROJECT_DIRS:\n            base_path = os.path.join(PROJECT_ROOT, pdir)\n            print(f\"\ud83d\udd0d Scanning {base_path}...\")\n            for ext in extensions:\n                files.extend(glob.glob(os.path.join(base_path, \"**\", ext), recursive=True))\n        \n        # Filter out node_modules and big dirs\n        files = [f for f in files if \"node_modules\" not in f and \".git\" not in f and \"dist\" not in f and \"venv\" not in f]\n        \n        total = len(files)\n        print(f\"\ud83d\ude80 Found {total} files. Starting parallel extraction with {MAX_WORKERS} workers...\")\n        \n        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n            futures = [executor.submit(self.extract_from_file, f) for f in files]\n            count = 0\n            for future in concurrent.futures.as_completed(futures):\n                count += 1\n                print(f\"[{count}/{total}] {future.result()}\")\n\nif __name__ == \"__main__\":\n    print(\"\ud83d\ude80 Initializing CodeGraphIndexer...\")\n    indexer = CodeGraphIndexer()\n    print(\"\u2705 Indexer Initialized. Starting Scan...\")\n    try:\n        indexer.scan_and_index()\n    finally:\n        indexer.close()\n        print(\"\ud83c\udfc1 Indexing Sequence Finished.\")\n"}
{"text": "File: route.ts\n\nimport { NextResponse } from 'next/server';\nimport { getDirectoryTree } from '@/lib/project-structure';\n\nexport async function GET(request: Request) {\n    const { searchParams } = new URL(request.url);\n    const path = searchParams.get('path') || '.';\n\n    try {\n        // We always fetch from root '.' recursively so the tree is fully populated\n        // The path param was intended for lazy loading but let's just send the whole tree for now \n        // as the File Explorer logic I wrote in Step 147 was:\n        // if (data.files) setFiles(prev => data.files) -- simply replacing.\n        const files = await getDirectoryTree('.');\n        return NextResponse.json({ files });\n    } catch (error) {\n        return NextResponse.json({ error: 'Failed to fetch files' }, { status: 500 });\n    }\n}\n"}
{"text": "File: test_chroma.py\n\nimport chromadb\nimport os\n\ntry:\n    print(\"Testing PersistentClient...\")\n    path = \"test_chroma_db\"\n    if not os.path.exists(path):\n        os.makedirs(path)\n        \n    client = chromadb.PersistentClient(path=path)\n    print(\"Success!\")\n    # cleanup\n    # import shutil\n    # shutil.rmtree(path)\nexcept Exception as e:\n    print(f\"FAILED: {e}\")\n    import traceback\n    traceback.print_exc()\n"}
{"text": "File: test_connection.py\n\nfrom neo4j import GraphDatabase\nimport time\n\nURI = \"bolt://localhost:7687\"\nUSER = \"neo4j\"\nPASSWORD = \"password123\"\n\ndef test_conn():\n    print(f\"\ud83d\udd17 Attempting connection to {URI}...\")\n    start = time.time()\n    try:\n        driver = GraphDatabase.driver(URI, auth=(USER, PASSWORD))\n        with driver.session() as session:\n            result = session.run(\"RETURN 1 as n\")\n            record = result.single()\n            print(f\"\u2705 Connection Successful! Result: {record['n']}\")\n        driver.close()\n    except Exception as e:\n        print(f\"\u274c Connection Failed: {e}\")\n    print(f\"\u23f1\ufe0f Elapsed: {time.time() - start:.2f}s\")\n\nif __name__ == \"__main__\":\n    test_conn()\n"}
{"text": "File: neural-core.ts\n\nimport { CreateMLCEngine, MLCEngine, InitProgressCallback } from \"@mlc-ai/web-llm\";\n\n// Configuration for the model\n// Llama-3-8B-Instruct is a good balance of performance and quality for M-series Macs\nconst SELECTED_MODEL = \"Llama-3-8B-Instruct-q4f32_1-MLC\";\n\nexport interface NeuralState {\n    isLoading: boolean;\n    progress: number;\n    text: string;\n    isReady: boolean;\n    error: string | null;\n    excitement: number; // 0-1, represents \"heart rate\" or generation speed\n    stress: number;   // 0-1, represents \"cognitive load\" or error rate \n    mode: 'LOCAL_LLM' | 'RAG_SERVER';\n    ragConnected: boolean;\n}\n\nexport type NeuralSubscriber = (state: NeuralState) => void;\n\nclass NeuralCore {\n    private engine: MLCEngine | null = null;\n    private state: NeuralState = {\n        isLoading: false,\n        progress: 0,\n        text: \"Idle\",\n        isReady: false,\n        error: null,\n        excitement: 0,\n        stress: 0,\n        mode: 'LOCAL_LLM',\n        ragConnected: false\n    };\n    private subscribers: NeuralSubscriber[] = [];\n\n    // Singleton instance\n    private static instance: NeuralCore;\n\n    private constructor() {\n        this.checkRagHealth();\n    }\n\n    public static getInstance(): NeuralCore {\n        if (!NeuralCore.instance) {\n            NeuralCore.instance = new NeuralCore();\n        }\n        return NeuralCore.instance;\n    }\n\n    public subscribe(callback: NeuralSubscriber) {\n        this.subscribers.push(callback);\n        callback(this.state); // Initial emission\n        return () => {\n            this.subscribers = this.subscribers.filter(sub => sub !== callback);\n        };\n    }\n\n    private updateState(updates: Partial<NeuralState>) {\n        this.state = { ...this.state, ...updates };\n        this.subscribers.forEach(sub => sub(this.state));\n    }\n\n    public async checkRagHealth() {\n        try {\n            const res = await fetch('http://localhost:8000/');\n            if (res.ok) {\n                this.updateState({ ragConnected: true });\n            } else {\n                this.updateState({ ragConnected: false });\n            }\n        } catch (e) {\n            this.updateState({ ragConnected: false });\n        }\n    }\n\n    public setMode(mode: 'LOCAL_LLM' | 'RAG_SERVER') {\n        this.updateState({ mode });\n        if (mode === 'RAG_SERVER') {\n            this.checkRagHealth();\n        }\n    }\n\n    public async initialize() {\n        if (this.engine || this.state.isLoading) return;\n\n        this.updateState({ isLoading: true, error: null, text: \"Initializing Neural Core...\" });\n\n        try {\n            const initProgressCallback: InitProgressCallback = (report) => {\n                const progress = parseFloat(report.progress.toFixed(2)); // handle very long decimals\n                this.updateState({\n                    progress: progress * 100,\n                    text: report.text\n                });\n            };\n\n            this.engine = await CreateMLCEngine(\n                SELECTED_MODEL,\n                { initProgressCallback }\n            );\n\n            this.updateState({\n                isLoading: false,\n                isReady: true,\n                text: \"Neural Core Online\",\n                progress: 100\n            });\n\n        } catch (error: any) {\n            console.error(\"Neural Core Init Error:\", error);\n            this.updateState({\n                isLoading: false,\n                error: error.message || \"Failed to initialize Neural Core\",\n                text: \"Initialization Failed\"\n            });\n        }\n    }\n\n    public async generateStream(\n        messages: { role: string; content: string }[],\n        onUpdate: (chunk: string) => void\n    ) {\n        if (this.state.mode === 'RAG_SERVER') {\n            return this.generateStreamRAG(messages, onUpdate);\n        }\n\n        if (!this.engine) {\n            throw new Error(\"Neural Core is not initialized\");\n        }\n\n        // Adapted for MLC format\n        // Ensure roles are supported (system, user, assistant)\n        const mlcMessages = messages.map(m => ({\n            role: m.role === \"model\" ? \"assistant\" : m.role, // Map 'model' to 'assistant'\n            content: m.content\n        })) as any[];\n\n        const stream = await this.engine.chat.completions.create({\n            messages: mlcMessages,\n            stream: true,\n        });\n\n        let lastTokenTime = performance.now();\n        let tokenCount = 0;\n\n        // Peak excitement at start of generation\n        this.updateState({ excitement: 0.8, stress: 0.2 });\n\n        for await (const chunk of stream) {\n            const content = chunk.choices[0]?.delta?.content || \"\";\n            if (content) {\n                const now = performance.now();\n                const delta = now - lastTokenTime;\n                lastTokenTime = now;\n                tokenCount++;\n\n                // Calculate excitement based on speed (lower delta = higher excitement)\n                // Normal speed is ~20-50ms per token.\n                // 20ms -> 1.0 excitement\n                // 100ms -> 0.2 excitement\n                const speed = 1000 / delta; // tokens per second roughly (or chars)\n                const targetExcitement = Math.min(Math.max(speed / 50, 0.3), 1.0);\n\n                // Stress increases if generation is very slow (high latency)\n                const runningStress = delta > 200 ? 0.7 : 0.1;\n\n                // Smooth updates to avoid jitter\n                this.updateState({\n                    excitement: targetExcitement,\n                    stress: runningStress\n                });\n\n                onUpdate(content);\n            }\n        }\n\n        // Cooldown\n        this.updateState({ excitement: 0.1, stress: 0 });\n    }\n\n    private async generateStreamRAG(\n        messages: { role: string; content: string }[],\n        onUpdate: (chunk: string) => void\n    ) {\n        const lastMessage = messages[messages.length - 1].content;\n\n        try {\n            this.updateState({ excitement: 0.5, stress: 0.1, text: \"Querying RAG...\" });\n\n            const response = await fetch('http://localhost:8000/api/chat', {\n                method: 'POST',\n                headers: { 'Content-Type': 'application/json' },\n                body: JSON.stringify({ message: lastMessage })\n            });\n\n            if (!response.body) throw new Error(\"No response body\");\n\n            const reader = response.body.getReader();\n            const decoder = new TextDecoder();\n\n            let lastTokenTime = performance.now();\n\n            while (true) {\n                const { done, value } = await reader.read();\n                if (done) break;\n\n                const chunk = decoder.decode(value);\n                const now = performance.now();\n                const delta = now - lastTokenTime;\n                lastTokenTime = now;\n\n                const speed = 1000 / delta;\n                const targetExcitement = Math.min(Math.max(speed / 30, 0.4), 1.0); // RAG might be slower/chunkier\n\n                this.updateState({\n                    excitement: targetExcitement,\n                });\n\n                onUpdate(chunk);\n            }\n\n            this.updateState({ excitement: 0.1, stress: 0, text: \"RAG Complete\" });\n\n        } catch (e: any) {\n            console.error(\"RAG Error:\", e);\n            this.updateState({ error: \"RAG Connection Failed\", stress: 1 });\n        }\n    }\n\n    public isReady() {\n        return this.state.isReady || (this.state.mode === 'RAG_SERVER' && this.state.ragConnected);\n    }\n\n    public async triggerIngest() {\n        try {\n            this.updateState({ text: \"Indexing Documents...\" });\n            const res = await fetch('http://localhost:8000/api/ingest', { method: 'POST' });\n            const data = await res.json();\n            this.updateState({ text: `Indexed ${data.chunks} chunks`, ragConnected: true });\n            return data;\n        } catch (e) {\n            this.updateState({ error: \"Ingestion Failed\" });\n        }\n    }\n}\n\nexport const neuralCore = NeuralCore.getInstance();\n"}
